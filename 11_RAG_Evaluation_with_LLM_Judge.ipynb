{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1758fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cea3125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM Response: LLM is Ready!\n",
      "\n",
      "✓ All models initialized successfully\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "judge_llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "test_response = llm.invoke(\"Say, 'LLM is Ready!'\")\n",
    "print(f\"✓ LLM Response: {test_response.content}\")\n",
    "print(\"\\n✓ All models initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c690b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 17 CloudFlow documentation documents.\n",
      "\n",
      "Document breakdown by category:\n",
      "   - Architecture: 3 documents\n",
      "   - Api: 4 documents\n",
      "   - Security: 2 documents\n",
      "   - Pricing: 2 documents\n",
      "   - Best_Practices: 3 documents\n",
      "   - Troubleshooting: 3 documents\n"
     ]
    }
   ],
   "source": [
    "cloudflow_docs = [\n",
    "    # ============================================================================\n",
    "    # ARCHITECTURE DOCUMENTS (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Architecture Overview\n",
    "\n",
    "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
    "\n",
    "The API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\n",
    "\n",
    "The Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\n",
    "\n",
    "The Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.\n",
    "\n",
    "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.\"\"\",\n",
    "        metadata={\"source\": \"architecture_overview\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Scaling Mechanisms\n",
    "\n",
    "CloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\n",
    "\n",
    "Horizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\n",
    "\n",
    "Vertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\n",
    "\n",
    "The platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.\n",
    "\n",
    "Load balancing distributes traffic across all available pods using a weighted round-robin algorithm. Health checks run every 10 seconds, and unhealthy pods are automatically removed from the rotation within 30 seconds.\"\"\",\n",
    "        metadata={\"source\": \"scaling_guide\", \"topic\": \"architecture\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow System Components\n",
    "\n",
    "CloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\n",
    "\n",
    "The Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\n",
    "\n",
    "The Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\n",
    "\n",
    "The Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\n",
    "\n",
    "The Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\n",
    "\n",
    "The Message Queue system, based on Apache Kafka, handles asynchronous communication between services with guaranteed message delivery and ordering.\"\"\",\n",
    "        metadata={\"source\": \"system_components\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # API DOCUMENTATION (4)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Authentication\n",
    "\n",
    "CloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\n",
    "\n",
    "OAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\n",
    "\n",
    "API Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\n",
    "\n",
    "To authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
    "\n",
    "API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.\"\"\",\n",
    "        metadata={\"source\": \"api_authentication\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow REST API Endpoints\n",
    "\n",
    "CloudFlow provides a comprehensive REST API with endpoints organized by resource type. All endpoints follow RESTful conventions and return JSON responses.\n",
    "\n",
    "Base URL: https://api.cloudflow.io/v1\n",
    "\n",
    "Resources endpoint: GET /api/v1/resources - List all resources with pagination (max 100 per page). Supports filtering by type, status, and creation date.\n",
    "\n",
    "Resource creation: POST /api/v1/resources - Create a new resource. Required fields: name (string), type (string), config (object). Returns 201 Created on success.\n",
    "\n",
    "Resource details: GET /api/v1/resources/{id} - Retrieve detailed information about a specific resource by ID.\n",
    "\n",
    "Resource update: PUT /api/v1/resources/{id} - Update an existing resource. Supports partial updates with PATCH /api/v1/resources/{id}.\n",
    "\n",
    "Resource deletion: DELETE /api/v1/resources/{id} - Delete a resource. Returns 204 No Content on success. Deleted resources are soft-deleted and can be recovered within 30 days.\n",
    "\n",
    "All list endpoints support query parameters: limit (default: 25, max: 100), offset (default: 0), sort (default: created_at), order (asc|desc).\"\"\",\n",
    "        metadata={\"source\": \"api_endpoints\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Rate Limiting\n",
    "\n",
    "CloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\n",
    "\n",
    "Standard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\n",
    "\n",
    "Premium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\n",
    "\n",
    "Enterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\n",
    "\n",
    "Rate limit headers are included in every response:\n",
    "- X-RateLimit-Limit: Maximum requests per hour\n",
    "- X-RateLimit-Remaining: Remaining requests in current window\n",
    "- X-RateLimit-Reset: Unix timestamp when the limit resets\n",
    "\n",
    "When rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.\n",
    "\n",
    "OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.\"\"\",\n",
    "        metadata={\"source\": \"api_rate_limits\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Error Codes\n",
    "\n",
    "CloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\n",
    "\n",
    "Authentication Errors:\n",
    "- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\n",
    "- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\n",
    "\n",
    "Client Errors:\n",
    "- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\n",
    "- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\n",
    "- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\n",
    "- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\n",
    "- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.\n",
    "\n",
    "Server Errors:\n",
    "- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\n",
    "- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\n",
    "- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\n",
    "\n",
    "Error Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}\"\"\",\n",
    "        metadata={\"source\": \"api_error_codes\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECURITY DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Security Features\n",
    "\n",
    "Security is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\n",
    "\n",
    "Encryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\n",
    "\n",
    "Network Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\n",
    "\n",
    "Access Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.\n",
    "\n",
    "Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\n",
    "\n",
    "Vulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.\"\"\",\n",
    "        metadata={\"source\": \"security_features\", \"topic\": \"security\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Compliance Standards\n",
    "\n",
    "CloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\n",
    "\n",
    "SOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\n",
    "\n",
    "GDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\n",
    "\n",
    "HIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\n",
    "\n",
    "ISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\n",
    "\n",
    "PCI DSS: For customers processing payment card data, CloudFlow provides PCI DSS Level 1 certified infrastructure. However, we recommend using dedicated payment processors rather than storing card data.\n",
    "\n",
    "Data Residency: CloudFlow supports data residency in US, EU, UK, and APAC regions to meet local regulatory requirements.\"\"\",\n",
    "        metadata={\"source\": \"compliance_standards\", \"topic\": \"security\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PRICING DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Pricing Tiers\n",
    "\n",
    "CloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\n",
    "\n",
    "Standard Tier ($99/month):\n",
    "- 1,000 API requests per hour\n",
    "- 100 GB storage included\n",
    "- 10 GB bandwidth per month\n",
    "- Community support via forums\n",
    "- 99.9% uptime SLA\n",
    "- Up to 5 team members\n",
    "\n",
    "Premium Tier ($499/month):\n",
    "- 10,000 API requests per hour\n",
    "- 1 TB storage included\n",
    "- 100 GB bandwidth per month\n",
    "- Email support with 24-hour response time\n",
    "- 99.95% uptime SLA\n",
    "- Up to 25 team members\n",
    "- Advanced monitoring and alerting\n",
    "- Custom domain support\n",
    "\n",
    "Enterprise Tier (Custom pricing):\n",
    "- Custom API rate limits (100,000+ requests/hour)\n",
    "- Unlimited storage and bandwidth\n",
    "- 24/7 phone and email support with 1-hour response time\n",
    "- 99.99% uptime SLA with service credits\n",
    "- Unlimited team members\n",
    "- Dedicated account manager\n",
    "- Custom integrations and professional services\n",
    "- Private cloud deployment options\n",
    "\n",
    "All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.\"\"\",\n",
    "        metadata={\"source\": \"pricing_tiers\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Billing Information\n",
    "\n",
    "Understanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\n",
    "\n",
    "Billing Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\n",
    "\n",
    "Usage-Based Charges: Beyond included quotas, additional usage is billed at:\n",
    "- API requests: $0.01 per 1,000 requests\n",
    "- Storage: $0.10 per GB per month\n",
    "- Bandwidth: $0.08 per GB\n",
    "- Backup retention (beyond 30 days): $0.05 per GB per month\n",
    "\n",
    "Payment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\n",
    "\n",
    "Invoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\n",
    "\n",
    "Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\n",
    "\n",
    "Free Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.\"\"\",\n",
    "        metadata={\"source\": \"billing_info\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # BEST PRACTICES DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Performance Optimization\n",
    "\n",
    "Following these best practices will help you achieve optimal performance from your CloudFlow applications.\n",
    "\n",
    "Caching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\n",
    "\n",
    "Request Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\n",
    "\n",
    "Connection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\n",
    "\n",
    "Query Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\n",
    "\n",
    "Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\n",
    "\n",
    "CDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\"\"\",\n",
    "        metadata={\"source\": \"performance_optimization\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Monitoring and Observability\n",
    "\n",
    "Effective monitoring ensures your CloudFlow applications remain healthy and performant.\n",
    "\n",
    "Metrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\n",
    "\n",
    "Custom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\n",
    "\n",
    "Alerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\n",
    "\n",
    "Distributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).\n",
    "\n",
    "Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\n",
    "\n",
    "Dashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.\"\"\",\n",
    "        metadata={\"source\": \"monitoring_observability\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Disaster Recovery\n",
    "\n",
    "CloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\n",
    "\n",
    "Backup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\n",
    "\n",
    "Point-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\n",
    "\n",
    "Multi-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\n",
    "\n",
    "Backup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.\n",
    "\n",
    "Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\n",
    "\n",
    "RTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.\"\"\",\n",
    "        metadata={\"source\": \"disaster_recovery\", \"topic\": \"best_practices\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TROUBLESHOOTING DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"Common CloudFlow Errors and Solutions\n",
    "\n",
    "This guide covers the most common errors encountered when using CloudFlow and their solutions.\n",
    "\n",
    "Error: \"Invalid API Key\" (401)\n",
    "Solution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\n",
    "\n",
    "Error: \"Rate Limit Exceeded\" (429)\n",
    "Solution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\n",
    "\n",
    "Error: \"Resource Not Found\" (404)\n",
    "Solution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).\n",
    "\n",
    "Error: \"Timeout\" (504)\n",
    "Solution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\n",
    "\n",
    "Error: \"Validation Error\" (422)\n",
    "Solution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.\"\"\",\n",
    "        metadata={\"source\": \"common_errors\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Debugging Guide\n",
    "\n",
    "When troubleshooting issues with CloudFlow, follow this systematic debugging approach.\n",
    "\n",
    "Step 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\n",
    "\n",
    "Step 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\n",
    "\n",
    "Step 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\n",
    "\n",
    "Step 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\n",
    "\n",
    "Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\n",
    "\n",
    "Step 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\n",
    "\n",
    "Step 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"\"\",\n",
    "        metadata={\"source\": \"debugging_guide\", \"topic\": \"troubleshooting\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Support Escalation Process\n",
    "\n",
    "Understanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\n",
    "\n",
    "Support Channels:\n",
    "- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\n",
    "- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\n",
    "- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\n",
    "- Slack Channel (Enterprise only): Direct access to engineering team\n",
    "\n",
    "Issue Severity Levels:\n",
    "- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\n",
    "- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\n",
    "- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\n",
    "- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\n",
    "\n",
    "Escalation Path: If your issue isn't resolved within SLA, it automatically escalates to the next support tier. Enterprise customers can request immediate escalation to engineering team.\n",
    "\n",
    "Required Information: Include account ID, request ID, error messages, timestamps, steps to reproduce, and expected vs actual behavior. Screenshots and API logs are helpful.\"\"\",\n",
    "        metadata={\"source\": \"support_escalation\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(cloudflow_docs)} CloudFlow documentation documents.\")\n",
    "print(\"\\nDocument breakdown by category:\")\n",
    "for topic in [\"architecture\", \"api\", \"security\", \"pricing\", \"best_practices\", \"troubleshooting\"]:\n",
    "    count = len([doc for doc in cloudflow_docs if doc.metadata['topic']==topic])\n",
    "    print(f\"   - {topic.title()}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd25603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 17 documents into 34 chunks\n",
      "\n",
      "Example chunk (first 200 chars): \n",
      "CloudFlow Architecture Overview\n",
      "\n",
      "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scala...\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\".\",\" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(cloudflow_docs)\n",
    "\n",
    "print(f\"Split {len(cloudflow_docs)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nExample chunk (first 200 chars): \\n{chunks[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749112d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n",
      "✓ FAISS vector store created with 34 document chunks\n",
      "✓ Vector store saved to './llm_judge_faiss'\n",
      "\n",
      "To reload later, use:\n",
      "  vectorstore = FAISS.load_local('./llm_judge_faiss', embeddings, allow_dangerous_deserialization=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating FAISS vector store...\")\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "vectorstore_path = \"./llm_judge_faiss\"\n",
    "vector_store.save_local(vectorstore_path)\n",
    "\n",
    "\n",
    "print(f\"✓ FAISS vector store created with {len(chunks)} document chunks\")\n",
    "print(f\"✓ Vector store saved to '{vectorstore_path}'\")\n",
    "print(f\"\\nTo reload later, use:\")\n",
    "print(f\"  vectorstore = FAISS.load_local('{vectorstore_path}', embeddings, allow_dangerous_deserialization=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdd038e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents.\n",
      "\n",
      "First relevant documents (200 chars): \n",
      "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independ...\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":4}\n",
    ")\n",
    "test_query = \"What is CloudFlow's uptime SLA?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents.\")\n",
    "print(f\"\\nFirst relevant documents (200 chars): \\n{retrieved_docs[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e196c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG chain created using LCEL\n",
      "\n",
      "Chain flow: Question → Retriever → format_docs → Prompt → LLM → Answer\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant for CloudFlow Platform documentation.\n",
    "Answer the question based on the following context. If you cannot answer based on\n",
    "the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Be concise and accurate. Include specific details like numbers, limits, and technical\n",
    "specifications when available in the context. Answer must be in 200 words\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_doc(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | format_doc, \"question\":RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✓ RAG chain created using LCEL\")\n",
    "print(\"\\nChain flow: Question → Retriever → format_docs → Prompt → LLM → Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a9d2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline with sample questions:\n",
      "\n",
      "1. Q: What is CloudFlow's uptime SLA?\n",
      "   A: CloudFlow's uptime SLA varies by pricing tier:\n",
      "\n",
      "- **Standard Tier**: Guarantees a 99.9% uptime SLA.  \n",
      "- **Premium Tier**: Guarantees a 99.95% uptime SLA.  \n",
      "- **Enterprise Tier**: Guarantees a 99.99% uptime SLA with service credits available in case of SLA violations.  \n",
      "\n",
      "For the Enterprise Tier, the platform also offers triple redundancy across availability zones to support the 99.99% uptime guarantee.\n",
      "2. Q: What authentication methods does CloudFlow support?\n",
      "   A: CloudFlow supports two authentication methods: **OAuth 2.0** and **API Keys**. \n",
      "\n",
      "1. **OAuth 2.0**:\n",
      "   - Recommended for user-facing applications.\n",
      "   - Supports the Authorization Code flow.\n",
      "   - Provides access tokens valid for 1 hour and refresh tokens valid for 30 days.\n",
      "   - Requires directing users to the authorization endpoint at `https://auth.cloudflow.io/oauth/authorize` with `client_id` and `redirect_uri` parameters.\n",
      "\n",
      "2. **API Keys**:\n",
      "   - Suitable for server-to-server communication and background jobs.\n",
      "   - API Key format: `cf_live_` followed by 32 alphanumeric characters.\n",
      "   - API keys do not expire unless explicitly revoked.\n",
      "   - To authenticate, include the API key in the `Authorization` header as: `Authorization: Bearer YOUR_API_KEY`.\n",
      "\n",
      "All requests must be made over HTTPS (TLS 1.3), and HTTP requests are rejected with a 403 error. Additionally, API keys can be scoped to specific permissions and restricted to IP addresses for enhanced security.\n",
      "3. Q: How do I handle rate limit errors?\n",
      "   A: To handle rate limit errors (HTTP 429), follow these steps:\n",
      "\n",
      "1. **Check Rate Limit Headers**: When you receive a 429 response, refer to the `X-RateLimit-Remaining` header to confirm the number of remaining requests and the `X-RateLimit-Reset` header for the timestamp when the rate limit resets.\n",
      "\n",
      "2. **Retry with Exponential Backoff**: Use the `Retry-After` header to wait for the indicated time before retrying. If the `Retry-After` header is unavailable, implement exponential backoff in your retry logic. Start with a 1-second delay, then double the wait time (e.g., 1s, 2s, 4s, etc.) for each subsequent retry while ensuring the backoff does not exceed reasonable limits.\n",
      "\n",
      "3. **Upgrade Your Plan**: If you consistently hit rate limits despite optimization, consider upgrading to a higher tier:\n",
      "   - **Standard Tier**: 1,000 requests/hour per API key (100 requests/minute).\n",
      "   - **Premium Tier**: 10,000 requests/hour per API key (500 requests/minute).\n",
      "   - **Enterprise Tier**: Custom limits starting at 100,000 requests/hour.\n",
      "\n",
      "4. **Use Batch Endpoints**: Combine multiple requests into batch operations where available to reduce the total request count.\n",
      "\n",
      "5. **Optimize Requests**: Review your API usage pattern to minimize unnecessary calls and ensure adherence to the request cap.\n",
      "\n",
      "By applying these steps, you can effectively manage rate limit errors and maintain stable interaction with CloudFlow.\n",
      "✓ RAG pipeline is working correctly!\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"What is CloudFlow's uptime SLA?\",\n",
    "    \"What authentication methods does CloudFlow support?\",\n",
    "    \"How do I handle rate limit errors?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG pipeline with sample questions:\\n\")\n",
    "\n",
    "for i, q in enumerate(test_questions,1):\n",
    "    ans = rag_chain.invoke(q)\n",
    "    print(f\"{i}. Q: {q}\")\n",
    "    print(f\"   A: {ans}\")\n",
    "\n",
    "print(\"✓ RAG pipeline is working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffbf34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
