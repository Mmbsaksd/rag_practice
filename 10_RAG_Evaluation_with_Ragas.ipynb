{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a1627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports successful!\n",
      "âœ“ Using LangChain 1.0+ with LCEL patterns\n",
      "âœ“ Ragas 0.3.7 evaluation framework loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from ragas import evaluate, EvaluationDataset, SingleTurnSample\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity,\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ“ All imports successful!\")\n",
    "print(\"âœ“ Using LangChain 1.0+ with LCEL patterns\")\n",
    "print(\"âœ“ Ragas 0.3.7 evaluation framework loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90422789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d792b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created 17 CloudFlow documentation documents\n",
      "\n",
      "Document breakdown by category: \n",
      "    -Architecture: 3 documents\n",
      "    -Api: 4 documents\n",
      "    -Security: 2 documents\n",
      "    -Pricing: 2 documents\n",
      "    -Best_Practices: 3 documents\n",
      "    -Troubleshooting: 3 documents\n"
     ]
    }
   ],
   "source": [
    "cloudflow_docs = [\n",
    "    # ============================================================================\n",
    "    # ARCHITECTURE DOCUMENTS (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Architecture Overview\n",
    "\n",
    "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
    "\n",
    "The API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\n",
    "\n",
    "The Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\n",
    "\n",
    "The Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand.\n",
    "\n",
    "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.\"\"\",\n",
    "        metadata={\"source\": \"architecture_overview\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Scaling Mechanisms\n",
    "\n",
    "CloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\n",
    "\n",
    "Horizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\n",
    "\n",
    "Vertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\n",
    "\n",
    "The platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling.\n",
    "\n",
    "Load balancing distributes traffic across all available pods using a weighted round-robin algorithm. Health checks run every 10 seconds, and unhealthy pods are automatically removed from the rotation within 30 seconds.\"\"\",\n",
    "        metadata={\"source\": \"scaling_guide\", \"topic\": \"architecture\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow System Components\n",
    "\n",
    "CloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\n",
    "\n",
    "The Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\n",
    "\n",
    "The Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\n",
    "\n",
    "The Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\n",
    "\n",
    "The Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.\n",
    "\n",
    "The Message Queue system, based on Apache Kafka, handles asynchronous communication between services with guaranteed message delivery and ordering.\"\"\",\n",
    "        metadata={\"source\": \"system_components\", \"topic\": \"architecture\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # API DOCUMENTATION (4)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Authentication\n",
    "\n",
    "CloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\n",
    "\n",
    "OAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\n",
    "\n",
    "API Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\n",
    "\n",
    "To authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
    "\n",
    "API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.\"\"\",\n",
    "        metadata={\"source\": \"api_authentication\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow REST API Endpoints\n",
    "\n",
    "CloudFlow provides a comprehensive REST API with endpoints organized by resource type. All endpoints follow RESTful conventions and return JSON responses.\n",
    "\n",
    "Base URL: https://api.cloudflow.io/v1\n",
    "\n",
    "Resources endpoint: GET /api/v1/resources - List all resources with pagination (max 100 per page). Supports filtering by type, status, and creation date.\n",
    "\n",
    "Resource creation: POST /api/v1/resources - Create a new resource. Required fields: name (string), type (string), config (object). Returns 201 Created on success.\n",
    "\n",
    "Resource details: GET /api/v1/resources/{id} - Retrieve detailed information about a specific resource by ID.\n",
    "\n",
    "Resource update: PUT /api/v1/resources/{id} - Update an existing resource. Supports partial updates with PATCH /api/v1/resources/{id}.\n",
    "\n",
    "Resource deletion: DELETE /api/v1/resources/{id} - Delete a resource. Returns 204 No Content on success. Deleted resources are soft-deleted and can be recovered within 30 days.\n",
    "\n",
    "All list endpoints support query parameters: limit (default: 25, max: 100), offset (default: 0), sort (default: created_at), order (asc|desc).\"\"\",\n",
    "        metadata={\"source\": \"api_endpoints\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Rate Limiting\n",
    "\n",
    "CloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\n",
    "\n",
    "Standard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\n",
    "\n",
    "Premium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\n",
    "\n",
    "Enterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\n",
    "\n",
    "Rate limit headers are included in every response:\n",
    "- X-RateLimit-Limit: Maximum requests per hour\n",
    "- X-RateLimit-Remaining: Remaining requests in current window\n",
    "- X-RateLimit-Reset: Unix timestamp when the limit resets\n",
    "\n",
    "When rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.\n",
    "\n",
    "OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.\"\"\",\n",
    "        metadata={\"source\": \"api_rate_limits\", \"topic\": \"api\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow API Error Codes\n",
    "\n",
    "CloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\n",
    "\n",
    "Authentication Errors:\n",
    "- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\n",
    "- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\n",
    "\n",
    "Client Errors:\n",
    "- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\n",
    "- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\n",
    "- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\n",
    "- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\n",
    "- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.\n",
    "\n",
    "Server Errors:\n",
    "- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\n",
    "- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\n",
    "- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\n",
    "\n",
    "Error Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}\"\"\",\n",
    "        metadata={\"source\": \"api_error_codes\", \"topic\": \"api\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SECURITY DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Security Features\n",
    "\n",
    "Security is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\n",
    "\n",
    "Encryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\n",
    "\n",
    "Network Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\n",
    "\n",
    "Access Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.\n",
    "\n",
    "Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\n",
    "\n",
    "Vulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours.\"\"\",\n",
    "        metadata={\"source\": \"security_features\", \"topic\": \"security\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Compliance Standards\n",
    "\n",
    "CloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\n",
    "\n",
    "SOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\n",
    "\n",
    "GDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\n",
    "\n",
    "HIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\n",
    "\n",
    "ISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.\n",
    "\n",
    "PCI DSS: For customers processing payment card data, CloudFlow provides PCI DSS Level 1 certified infrastructure. However, we recommend using dedicated payment processors rather than storing card data.\n",
    "\n",
    "Data Residency: CloudFlow supports data residency in US, EU, UK, and APAC regions to meet local regulatory requirements.\"\"\",\n",
    "        metadata={\"source\": \"compliance_standards\", \"topic\": \"security\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # PRICING DOCUMENTATION (2)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Pricing Tiers\n",
    "\n",
    "CloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\n",
    "\n",
    "Standard Tier ($99/month):\n",
    "- 1,000 API requests per hour\n",
    "- 100 GB storage included\n",
    "- 10 GB bandwidth per month\n",
    "- Community support via forums\n",
    "- 99.9% uptime SLA\n",
    "- Up to 5 team members\n",
    "\n",
    "Premium Tier ($499/month):\n",
    "- 10,000 API requests per hour\n",
    "- 1 TB storage included\n",
    "- 100 GB bandwidth per month\n",
    "- Email support with 24-hour response time\n",
    "- 99.95% uptime SLA\n",
    "- Up to 25 team members\n",
    "- Advanced monitoring and alerting\n",
    "- Custom domain support\n",
    "\n",
    "Enterprise Tier (Custom pricing):\n",
    "- Custom API rate limits (100,000+ requests/hour)\n",
    "- Unlimited storage and bandwidth\n",
    "- 24/7 phone and email support with 1-hour response time\n",
    "- 99.99% uptime SLA with service credits\n",
    "- Unlimited team members\n",
    "- Dedicated account manager\n",
    "- Custom integrations and professional services\n",
    "- Private cloud deployment options\n",
    "\n",
    "All tiers include: SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides 15% discount.\"\"\",\n",
    "        metadata={\"source\": \"pricing_tiers\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Billing Information\n",
    "\n",
    "Understanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\n",
    "\n",
    "Billing Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\n",
    "\n",
    "Usage-Based Charges: Beyond included quotas, additional usage is billed at:\n",
    "- API requests: $0.01 per 1,000 requests\n",
    "- Storage: $0.10 per GB per month\n",
    "- Bandwidth: $0.08 per GB\n",
    "- Backup retention (beyond 30 days): $0.05 per GB per month\n",
    "\n",
    "Payment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\n",
    "\n",
    "Invoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms.\n",
    "\n",
    "Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\n",
    "\n",
    "Free Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage.\"\"\",\n",
    "        metadata={\"source\": \"billing_info\", \"topic\": \"pricing\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # BEST PRACTICES DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Performance Optimization\n",
    "\n",
    "Following these best practices will help you achieve optimal performance from your CloudFlow applications.\n",
    "\n",
    "Caching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\n",
    "\n",
    "Request Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\n",
    "\n",
    "Connection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\n",
    "\n",
    "Query Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally.\n",
    "\n",
    "Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\n",
    "\n",
    "CDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.\"\"\",\n",
    "        metadata={\"source\": \"performance_optimization\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Monitoring and Observability\n",
    "\n",
    "Effective monitoring ensures your CloudFlow applications remain healthy and performant.\n",
    "\n",
    "Metrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\n",
    "\n",
    "Custom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\n",
    "\n",
    "Alerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\n",
    "\n",
    "Distributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%).\n",
    "\n",
    "Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\n",
    "\n",
    "Dashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration.\"\"\",\n",
    "        metadata={\"source\": \"monitoring_observability\", \"topic\": \"best_practices\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Disaster Recovery\n",
    "\n",
    "CloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\n",
    "\n",
    "Backup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\n",
    "\n",
    "Point-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\n",
    "\n",
    "Multi-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\n",
    "\n",
    "Backup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.\n",
    "\n",
    "Export Capabilities: Export your data anytime in JSON, CSV, or Parquet format. Full exports are available via the /api/v1/export endpoint. Large exports (>10 GB) are delivered to your S3 bucket.\n",
    "\n",
    "RTO and RPO: CloudFlow guarantees Recovery Time Objective (RTO) of 4 hours and Recovery Point Objective (RPO) of 1 hour for Enterprise tier. Contact support to initiate disaster recovery procedures.\"\"\",\n",
    "        metadata={\"source\": \"disaster_recovery\", \"topic\": \"best_practices\", \"difficulty\": \"advanced\"}\n",
    "    ),\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TROUBLESHOOTING DOCUMENTATION (3)\n",
    "    # ============================================================================\n",
    "    Document(\n",
    "        page_content=\"\"\"Common CloudFlow Errors and Solutions\n",
    "\n",
    "This guide covers the most common errors encountered when using CloudFlow and their solutions.\n",
    "\n",
    "Error: \"Invalid API Key\" (401)\n",
    "Solution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\n",
    "\n",
    "Error: \"Rate Limit Exceeded\" (429)\n",
    "Solution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\n",
    "\n",
    "Error: \"Resource Not Found\" (404)\n",
    "Solution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).\n",
    "\n",
    "Error: \"Timeout\" (504)\n",
    "Solution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\n",
    "\n",
    "Error: \"Validation Error\" (422)\n",
    "Solution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.\"\"\",\n",
    "        metadata={\"source\": \"common_errors\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Debugging Guide\n",
    "\n",
    "When troubleshooting issues with CloudFlow, follow this systematic debugging approach.\n",
    "\n",
    "Step 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\n",
    "\n",
    "Step 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\n",
    "\n",
    "Step 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\n",
    "\n",
    "Step 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources\n",
    "\n",
    "Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\n",
    "\n",
    "Step 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\n",
    "\n",
    "Step 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.\"\"\",\n",
    "        metadata={\"source\": \"debugging_guide\", \"topic\": \"troubleshooting\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    \n",
    "    Document(\n",
    "        page_content=\"\"\"CloudFlow Support Escalation Process\n",
    "\n",
    "Understanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\n",
    "\n",
    "Support Channels:\n",
    "- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\n",
    "- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\n",
    "- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\n",
    "- Slack Channel (Enterprise only): Direct access to engineering team\n",
    "\n",
    "Issue Severity Levels:\n",
    "- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\n",
    "- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\n",
    "- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\n",
    "- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours\n",
    "\n",
    "Escalation Path: If your issue isn't resolved within SLA, it automatically escalates to the next support tier. Enterprise customers can request immediate escalation to engineering team.\n",
    "\n",
    "Required Information: Include account ID, request ID, error messages, timestamps, steps to reproduce, and expected vs actual behavior. Screenshots and API logs are helpful.\"\"\",\n",
    "        metadata={\"source\": \"support_escalation\", \"topic\": \"troubleshooting\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\" Created {len(cloudflow_docs)} CloudFlow documentation documents\")\n",
    "print(\"\\nDocument breakdown by category: \")\n",
    "for topic in [\"architecture\", \"api\", \"security\", \"pricing\", \"best_practices\", \"troubleshooting\"]:\n",
    "    count = len([doc for doc in cloudflow_docs if doc.metadata[\"topic\"]==topic])\n",
    "    print(f\"    -{topic.title()}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac281459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Document: \n",
      "================================================================================\n",
      "Content preview(first 300 chars): \n",
      "CloudFlow Architecture Overview\n",
      "\n",
      "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\n",
      "\n",
      "The API Gateway layer handles all incoming requests using OAuth 2.0 authenticat...\n",
      "\n",
      "Metadata: {'source': 'architecture_overview', 'topic': 'architecture', 'difficulty': 'intermediate'}\n",
      "================================================================================\n",
      "\n",
      "Dataset statistics:\n",
      "   Total documents: 17\n",
      "   Total charactor: 20987\n",
      "   Average documents length: 1234\n"
     ]
    }
   ],
   "source": [
    "sample_doc = cloudflow_docs[0]\n",
    "\n",
    "print(\"Sample Document: \")\n",
    "print(\"=\"*80)\n",
    "print(f\"Content preview(first 300 chars): \\n{sample_doc.page_content[:300]}...\\n\")\n",
    "print(f\"Metadata: {sample_doc.metadata}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_char = sum(len(doc.page_content) for doc in cloudflow_docs)\n",
    "avg_char = total_char/ len(cloudflow_docs)\n",
    "\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"   Total documents: {len(cloudflow_docs)}\")\n",
    "print(f\"   Total charactor: {total_char}\")\n",
    "print(f\"   Average documents length: {int(avg_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb560b8b",
   "metadata": {},
   "source": [
    "### **Basic RAG Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aaa9d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Split 17 documents into 34 chunks\n",
      "\n",
      "Chunk statics:\n",
      "   Average chunk size: 616 characotrs\n",
      "   Max chunk size: 105 charactors\n",
      "   Max chunk size: 1012 charactors\n",
      "\n",
      "Sample chunk:\n",
      "Content: CloudFlow Architecture Overview\n",
      "\n",
      "CloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scala...\n",
      "Metadata: {'source': 'architecture_overview', 'topic': 'architecture', 'difficulty': 'intermediate'}\n"
     ]
    }
   ],
   "source": [
    "#Step 1:\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128,\n",
    "    separators = [\"\\n\\n\",\"\\n\", \".\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(cloudflow_docs)\n",
    "\n",
    "print(f\"âœ“ Split {len(cloudflow_docs)} documents into {len(chunks)} chunks\")\n",
    "print(f\"\\nChunk statics:\")\n",
    "chunks_length = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "print(f\"   Average chunk size: {int(np.mean(chunks_length))} characotrs\")\n",
    "print(f\"   Max chunk size: {min(chunks_length)} charactors\")\n",
    "print(f\"   Max chunk size: {max(chunks_length)} charactors\")\n",
    "\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65620676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store...\n",
      "   Embedding 34 chunks\n",
      "âœ“ FAISS vector store created and saved to './ragas_evaluation_faiss'\n",
      "âœ“ Indexed 34 document chunks\n",
      "\n",
      "Vector store can be reloaded with:\n",
      "  vectorstore = FAISS.load_local('./ragas_evaluation_faiss', embeddings, allow_dangerous_deserialization=True)\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Create embeddings and vector store\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    ")\n",
    "\n",
    "print(\"Creating FAISS vector store...\")\n",
    "print(f\"   Embedding {len(chunks)} chunks\")\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    chunks, embeddings\n",
    ")\n",
    "vectorstore_path = \"./ragas_evaluation_faiss\"\n",
    "vector_store.save_local(vectorstore_path)\n",
    "\n",
    "\n",
    "print(f\"âœ“ FAISS vector store created and saved to '{vectorstore_path}'\")\n",
    "print(f\"âœ“ Indexed {len(chunks)} document chunks\")\n",
    "print(\"\\nVector store can be reloaded with:\")\n",
    "print(f\"  vectorstore = FAISS.load_local('{vectorstore_path}', embeddings, allow_dangerous_deserialization=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02e32c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Retriever configured successfully\n",
      "  Search type: similarity\n",
      "  Number of results (k): 4\n",
      "\n",
      "Testing retriever with query: 'What is CloudFlow's uptime SLA?'\n",
      "âœ“ Retrieved 4 documents\n",
      "\n",
      "First retrieved chunk preview:\n",
      "CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics....\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":4}\n",
    ")\n",
    "\n",
    "print(\"âœ“ Retriever configured successfully\")\n",
    "print(\"  Search type: similarity\")\n",
    "print(\"  Number of results (k): 4\")\n",
    "\n",
    "test_query = \"What is CloudFlow's uptime SLA?\"\n",
    "print(f\"\\nTesting retriever with query: '{test_query}'\")\n",
    "\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "print(f\"âœ“ Retrieved {len(retrieved_docs)} documents\")\n",
    "print(f\"\\nFirst retrieved chunk preview:\")\n",
    "print(f\"{retrieved_docs[0].page_content[:250]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07319521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Language model configured\n",
      "  Model: gpt-4o-mini\n",
      "  Temperature: 0 (deterministic)\n",
      "  Max tokens: 500\n",
      "\n",
      "LLM test: LLM is ready! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")\n",
    "print(\"âœ“ Language model configured\")\n",
    "print(\"  Model: gpt-4o-mini\")\n",
    "print(\"  Temperature: 0 (deterministic)\")\n",
    "print(\"  Max tokens: 500\")\n",
    "\n",
    "test_response = llm.invoke(\"Say 'LLM is ready!'\")\n",
    "print(f\"\\nLLM test: {test_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd12df64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prompt template created\n",
      "\n",
      "Prompt structure:\n",
      "  1. System instruction (CloudFlow assistant role)\n",
      "  2. Context from retrieved documents\n",
      "  3. User question\n",
      "  4. Answer placeholder\n"
     ]
    }
   ],
   "source": [
    "#Create prompt template\n",
    "template = \"\"\"You are a helpful assistant for CloudFlow Platform documentation.\n",
    "Answer the question based on the following context. If you cannot answer based on\n",
    "the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "Be concise and accurate. Include specific details like numbers, limits, and technical\n",
    "specifications when available in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "print(\"âœ“ Prompt template created\")\n",
    "print(\"\\nPrompt structure:\")\n",
    "print(\"  1. System instruction (CloudFlow assistant role)\")\n",
    "print(\"  2. Context from retrieved documents\")\n",
    "print(\"  3. User question\")\n",
    "print(\"  4. Answer placeholder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af93db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain created using LCEL\n",
      "\n",
      "Chain flow:\n",
      "  1. Question â†’ Retriever â†’ Get relevant documents\n",
      "  2. Documents â†’ format_docs â†’ Create context string\n",
      "  3. Context + Question â†’ Prompt template\n",
      "  4. Prompt â†’ LLM â†’ Generate answer\n",
      "  5. LLM output â†’ StrOutputParser â†’ Extract text\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Building chain\n",
    "def doc_format(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | doc_format, \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain created using LCEL\")\n",
    "print(\"\\nChain flow:\")\n",
    "print(\"  1. Question â†’ Retriever â†’ Get relevant documents\")\n",
    "print(\"  2. Documents â†’ format_docs â†’ Create context string\")\n",
    "print(\"  3. Context + Question â†’ Prompt template\")\n",
    "print(\"  4. Prompt â†’ LLM â†’ Generate answer\")\n",
    "print(\"  5. LLM output â†’ StrOutputParser â†’ Extract text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b759c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG pipeline with sample questions:\n",
      "\n",
      "================================================================================\n",
      "Question 1: What is CloudFlow's uptime SLA?\n",
      "Answer: CloudFlow's uptime SLA is 99.99% for the Enterprise tier and 99.95% for the Premium tier. The Standard tier offers a 99.9% uptime SLA.\n",
      "--------------------------------------------------------------------------------\n",
      "Question 2: How do I authenticate with CloudFlow APIs?\n",
      "Answer: You can authenticate with CloudFlow APIs using either OAuth 2.0 or API Keys:\n",
      "\n",
      "1. **OAuth 2.0**: Recommended for user-facing applications.  \n",
      "   - Use the Authorization Code flow to obtain access tokens (valid for 1 hour) and refresh tokens (valid for 30 days).  \n",
      "   - Direct users to the authorization endpoint at `https://auth.cloudflow.io/oauth/authorize` with your `client_id` and `redirect_uri` parameters.\n",
      "\n",
      "2. **API Keys**: Ideal for server-to-server communication and background jobs.  \n",
      "   - API keys have the format `cf_live_` followed by 32 alphanumeric characters.  \n",
      "   - API keys do not expire unless explicitly revoked.  \n",
      "   - Include the API key in the Authorization header: `Authorization: Bearer YOUR_API_KEY`.  \n",
      "   - API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security.\n",
      "\n",
      "All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error.\n",
      "--------------------------------------------------------------------------------\n",
      "Question 3: What are the pricing tiers?\n",
      "Answer: CloudFlow offers three pricing tiers:\n",
      "\n",
      "1. **Standard Tier** ($99/month):  \n",
      "   - 1,000 API requests per hour  \n",
      "   - 100 GB storage included  \n",
      "   - 10 GB bandwidth per month  \n",
      "   - Community support via forums  \n",
      "   - 99.9% uptime SLA  \n",
      "   - Up to 5 team members  \n",
      "\n",
      "2. **Premium Tier** ($499/month):  \n",
      "   - 10,000 API requests per hour  \n",
      "   - 1 TB storage included  \n",
      "   - 100 GB bandwidth per month  \n",
      "   - Email support with 24-hour response time  \n",
      "   - 99.95% uptime SLA  \n",
      "   - Up to 25 team members  \n",
      "   - Advanced monitoring and alerting  \n",
      "   - Custom domain support  \n",
      "\n",
      "3. **Enterprise Tier** (Custom pricing):  \n",
      "   - Custom API rate limits (100,000+ requests/hour)  \n",
      "   - Unlimited storage and bandwidth  \n",
      "   - 24/7 phone and email support with 1-hour response time  \n",
      "   - 99.99% uptime SLA with service credits  \n",
      "   - Unlimited team members  \n",
      "   - Dedicated account manager  \n",
      "   - Custom integrations and professional services  \n",
      "   - Private cloud deployment options  \n",
      "\n",
      "All tiers include SSL certificates, daily backups, API access, and dashboard analytics. Annual billing provides a 15% discount.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Test RAG pipeline\n",
    "test_questions = [\n",
    "    \"What is CloudFlow's uptime SLA?\",\n",
    "    \"How do I authenticate with CloudFlow APIs?\",\n",
    "    \"What are the pricing tiers?\"\n",
    "]\n",
    "\n",
    "print(\"Testing RAG pipeline with sample questions:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, q in enumerate(test_questions,1):\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    answer = rag_chain.invoke(q)\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddfe673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 18 test questions with ground truth answers\n",
      "\n",
      "Question breakdown by category:\n",
      "  - Simple Factual: 5 questions\n",
      "  - Multi-Fact: 4 questions\n",
      "  - Procedural: 3 questions\n",
      "  - Comparison: 2 questions\n",
      "  - Troubleshooting: 2 questions\n",
      "  - Edge Cases: 2 questions\n"
     ]
    }
   ],
   "source": [
    "#Step 6: evaluation\n",
    "\n",
    "test_cases = [\n",
    "    # ========== SIMPLE FACTUAL (5) ==========\n",
    "    {\n",
    "        \"question\": \"What is CloudFlow's uptime SLA?\",\n",
    "        \"ground_truth\": \"CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What authentication protocol does CloudFlow use?\",\n",
    "        \"ground_truth\": \"CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the service mesh technology used by CloudFlow?\",\n",
    "        \"ground_truth\": \"CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What compliance standards does CloudFlow support?\",\n",
    "        \"ground_truth\": \"CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long are CloudFlow audit logs retained?\",\n",
    "        \"ground_truth\": \"CloudFlow audit logs are immutable and retained for 2 years.\"\n",
    "    },\n",
    "    \n",
    "    # ========== MULTI-FACT (4) ==========\n",
    "    {\n",
    "        \"question\": \"What are the three main layers of CloudFlow architecture?\",\n",
    "        \"ground_truth\": \"The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are CloudFlow's pricing tiers and their API rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What HTTP status codes indicate authentication failures in CloudFlow API?\",\n",
    "        \"ground_truth\": \"401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What auto-scaling metrics does CloudFlow monitor?\",\n",
    "        \"ground_truth\": \"CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.\"\n",
    "    },\n",
    "    \n",
    "    # ========== PROCEDURAL (3) ==========\n",
    "    {\n",
    "        \"question\": \"How do I authenticate with CloudFlow APIs using an API key?\",\n",
    "        \"ground_truth\": \"Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I handle rate limit errors in CloudFlow?\",\n",
    "        \"ground_truth\": \"When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What steps should I follow to optimize CloudFlow API performance?\",\n",
    "        \"ground_truth\": \"Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.\"\n",
    "    },\n",
    "    \n",
    "    # ========== COMPARISON (2) ==========\n",
    "    {\n",
    "        \"question\": \"What's the difference between Standard and Premium tier rate limits?\",\n",
    "        \"ground_truth\": \"Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?\",\n",
    "        \"ground_truth\": \"OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.\"\n",
    "    },\n",
    "    \n",
    "    # ========== TROUBLESHOOTING (2) ==========\n",
    "    {\n",
    "        \"question\": \"What should I do if I receive a 504 timeout error?\",\n",
    "        \"ground_truth\": \"Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I debug slow API response times in CloudFlow?\",\n",
    "        \"ground_truth\": \"Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.\"\n",
    "    },\n",
    "    \n",
    "    # ========== EDGE CASES (2) ==========\n",
    "    {\n",
    "        \"question\": \"What happens if I use an expired OAuth token?\",\n",
    "        \"ground_truth\": \"If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does CloudFlow support blockchain integration?\",\n",
    "        \"ground_truth\": \"I don't have enough information to answer that question.\"  # Tests 'I don't know' handling\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"âœ“ Created {len(test_cases)} test questions with ground truth answers\\n\")\n",
    "print(\"Question breakdown by category:\")\n",
    "print(\"  - Simple Factual: 5 questions\")\n",
    "print(\"  - Multi-Fact: 4 questions\")\n",
    "print(\"  - Procedural: 3 questions\")\n",
    "print(\"  - Comparison: 2 questions\")\n",
    "print(\"  - Troubleshooting: 2 questions\")\n",
    "print(\"  - Edge Cases: 2 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b98ed3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers and capturing contexts for all test questions...\n",
      "\n",
      "[1/18] Processing: What is CloudFlow's uptime SLA?...\n",
      "[2/18] Processing: What authentication protocol does CloudFlow use?...\n",
      "[3/18] Processing: What is the service mesh technology used by CloudFlow?...\n",
      "[4/18] Processing: What compliance standards does CloudFlow support?...\n",
      "[5/18] Processing: How long are CloudFlow audit logs retained?...\n",
      "[6/18] Processing: What are the three main layers of CloudFlow architecture?...\n",
      "[7/18] Processing: What are CloudFlow's pricing tiers and their API rate limits...\n",
      "[8/18] Processing: What HTTP status codes indicate authentication failures in C...\n",
      "[9/18] Processing: What auto-scaling metrics does CloudFlow monitor?...\n",
      "[10/18] Processing: How do I authenticate with CloudFlow APIs using an API key?...\n",
      "[11/18] Processing: How do I handle rate limit errors in CloudFlow?...\n",
      "[12/18] Processing: What steps should I follow to optimize CloudFlow API perform...\n",
      "[13/18] Processing: What's the difference between Standard and Premium tier rate...\n",
      "[14/18] Processing: How does OAuth 2.0 authentication differ from API key authen...\n",
      "[15/18] Processing: What should I do if I receive a 504 timeout error?...\n",
      "[16/18] Processing: How do I debug slow API response times in CloudFlow?...\n",
      "[17/18] Processing: What happens if I use an expired OAuth token?...\n",
      "[18/18] Processing: Does CloudFlow support blockchain integration?...\n",
      "\n",
      "   Generated 18 answer-context pairs\n",
      "\n",
      "Dataset structure: \n",
      "   - user_imput: 18 questions\n",
      "  - reference: 18 ground truth answers\n",
      "  - response: 18 RAG-generated answers\n",
      "  - retrieved_contexts: 18 context lists\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating answers and capturing contexts for all test questions...\\n\")\n",
    "evaluation_data = {\n",
    "    \"user_input\":[],\n",
    "    \"reference\":[],\n",
    "    \"response\":[],\n",
    "    \"retrieved_contexts\":[]\n",
    "}\n",
    "for i, test_case in enumerate(test_cases,1):\n",
    "    question = test_case['question']\n",
    "    referance = test_case['ground_truth']\n",
    "\n",
    "    print(f\"[{i}/{len(test_cases)}] Processing: {question[:60]}...\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    context = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "    evaluation_data['user_input'].append(question)\n",
    "    evaluation_data['reference'].append(referance)\n",
    "    evaluation_data[\"response\"].append(answer)\n",
    "    evaluation_data['retrieved_contexts'].append(context)\n",
    "\n",
    "print(f\"\\n   Generated {len(evaluation_data['user_input'])} answer-context pairs\")\n",
    "print(\"\\nDataset structure: \")\n",
    "print(f\"   - user_imput: {len(evaluation_data['user_input'])} questions\")\n",
    "print(f\"  - reference: {len(evaluation_data['reference'])} ground truth answers\")\n",
    "print(f\"  - response: {len(evaluation_data['response'])} RAG-generated answers\")\n",
    "print(f\"  - retrieved_contexts: {len(evaluation_data['retrieved_contexts'])} context lists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f21663c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Converted to Ragas EvaluationDataset format\n",
      "  Total samples: 18\n",
      "\n",
      "Sample preview:\n",
      "  Question: What is CloudFlow's uptime SLA?\n",
      "  Answer: CloudFlow's uptime SLA is 99.99% for the Enterprise tier and 99.95% for the Premium tier. The Standa...\n",
      "  Contexts: 4 chunks retrieved\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=evaluation_data['user_input'][i],\n",
    "        response=evaluation_data['response'][i],\n",
    "        retrieved_contexts=evaluation_data['retrieved_contexts'][i],\n",
    "        reference=evaluation_data['reference'][i]\n",
    "    ) for i in range(len(evaluation_data['user_input']))\n",
    "]\n",
    "eval_dataset = EvaluationDataset(samples=samples)\n",
    "\n",
    "print(\"âœ“ Converted to Ragas EvaluationDataset format\")\n",
    "print(f\"  Total samples: {len(eval_dataset)}\")\n",
    "print(f\"\\nSample preview:\")\n",
    "print(f\"  Question: {samples[0].user_input}\")\n",
    "print(f\"  Answer: {samples[0].response[:100]}...\")\n",
    "print(f\"  Contexts: {len(samples[0].retrieved_contexts)} chunks retrieved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "997a4073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFIGURING LLM & EMBEDDINGS (WITH FIX)\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Creating SEPARATE evaluator LLM with increased limits...\n",
      "âœ… Evaluator LLM configured:\n",
      "   Model: gpt-4o\n",
      "   Temperature: 0 (deterministic)\n",
      "   Max tokens: 2000 (sufficient for Ragas evaluations)\n",
      "   Request timeout: 120 seconds\n",
      "\n",
      "ðŸ”§ Creating embeddings model...\n",
      "âœ… Embeddings configured:\n",
      "   Model: text-embedding-3-small\n",
      "   Dimensions: 1536\n",
      "\n",
      "ðŸ”§ Wrapping LLM and embeddings for Ragas...\n",
      "âœ… Wrappers created successfully\n",
      "âœ“ Configured 6 Ragas metrics (v0.3.9) - All non-multimodal metrics\n",
      "\n",
      "Metric Details:\n",
      "================================================================================\n",
      "\n",
      "1. FAITHFULNESS\n",
      "   - Measures: Answer groundedness in retrieved context\n",
      "   - Type: LLM-based (uses GPT-4o-mini for verification)\n",
      "   - Initialization: Faithfulness(llm=ragas_llm)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~2-3s per question\n",
      "   - Why: Prevents hallucinations and ensures factual accuracy\n",
      "\n",
      "2. ANSWER RELEVANCY (Response Relevancy)\n",
      "   - Measures: Semantic relevance of answer to question\n",
      "   - Type: Embedding-based (uses cosine similarity)\n",
      "   - Initialization: AnswerRelevancy(embeddings=ragas_embeddings)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~0.5s per question\n",
      "   - Why: Ensures on-topic, focused responses\n",
      "\n",
      "3. CONTEXT PRECISION\n",
      "   - Measures: Ranking quality of retrieved chunks\n",
      "   - Type: LLM-based ground truth comparison\n",
      "   - Initialization: ContextPrecision(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1s per question\n",
      "   - Why: Evaluates retrieval algorithm effectiveness\n",
      "\n",
      "4. CONTEXT RECALL\n",
      "   - Measures: Completeness of retrieved information\n",
      "   - Type: LLM-based ground truth comparison\n",
      "   - Initialization: ContextRecall(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1s per question\n",
      "   - Why: Ensures no critical information is missed\n",
      "\n",
      "5. CONTEXT ENTITY RECALL\n",
      "   - Measures: Key entities from ground truth present in retrieved contexts\n",
      "   - Type: LLM-based entity extraction and matching\n",
      "   - Initialization: ContextEntityRecall(llm=ragas_llm)\n",
      "   - Target: >0.6\n",
      "   - Speed: ~1-2s per question\n",
      "   - Why: Ensures important named entities are retrieved\n",
      "\n",
      "6. NOISE SENSITIVITY\n",
      "   - Measures: Robustness to irrelevant/noisy context\n",
      "   - Type: LLM-based evaluation with added noise\n",
      "   - Initialization: NoiseSensitivity(llm=ragas_llm)\n",
      "   - Target: >0.7\n",
      "   - Speed: ~2-3s per question\n",
      "   - Why: Tests if system can ignore distractors\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Œ Note on Ragas 0.3.9:\n",
      "   All 6 non-multimodal metrics are now configured.\n",
      "   Metrics are initialized as classes with wrapped LLM/embeddings.\n",
      "   LangChain components must be wrapped using Ragas wrapper classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFIGURING LLM & EMBEDDINGS (WITH FIX)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ”§ Creating SEPARATE evaluator LLM with increased limits...\")\n",
    "\n",
    "evaluator_llm =AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0,\n",
    "    max_tokens=3000,\n",
    "    request_timeout=480,\n",
    "    n=3\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluator LLM configured:\")\n",
    "print(f\"   Model: gpt-4o\")\n",
    "print(f\"   Temperature: 0 (deterministic)\")\n",
    "print(f\"   Max tokens: 2000 (sufficient for Ragas evaluations)\")\n",
    "print(f\"   Request timeout: 120 seconds\")\n",
    "\n",
    "\n",
    "print(\"\\nðŸ”§ Creating embeddings model...\")\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Embeddings configured:\")\n",
    "print(f\"   Model: text-embedding-3-small\")\n",
    "print(f\"   Dimensions: 1536\")\n",
    "\n",
    "print(\"\\nðŸ”§ Wrapping LLM and embeddings for Ragas...\")\n",
    "ragas_llm = LangchainLLMWrapper(evaluator_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "print(\"âœ… Wrappers created successfully\")\n",
    "\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevancy,\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    AnswerRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "    ContextPrecision(llm=ragas_llm),\n",
    "    ContextRecall(llm=ragas_llm),\n",
    "    ContextEntityRecall(llm=ragas_llm),\n",
    "    NoiseSensitivity(llm=ragas_llm)\n",
    "]\n",
    "\n",
    "print(\"âœ“ Configured 6 Ragas metrics (v0.3.9) - All non-multimodal metrics\\n\")\n",
    "print(\"Metric Details:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. FAITHFULNESS\")\n",
    "print(\"   - Measures: Answer groundedness in retrieved context\")\n",
    "print(\"   - Type: LLM-based (uses GPT-4o-mini for verification)\")\n",
    "print(\"   - Initialization: Faithfulness(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~2-3s per question\")\n",
    "print(\"   - Why: Prevents hallucinations and ensures factual accuracy\")\n",
    "\n",
    "print(\"\\n2. ANSWER RELEVANCY (Response Relevancy)\")\n",
    "print(\"   - Measures: Semantic relevance of answer to question\")\n",
    "print(\"   - Type: Embedding-based (uses cosine similarity)\")\n",
    "print(\"   - Initialization: AnswerRelevancy(embeddings=ragas_embeddings)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~0.5s per question\")\n",
    "print(\"   - Why: Ensures on-topic, focused responses\")\n",
    "\n",
    "print(\"\\n3. CONTEXT PRECISION\")\n",
    "print(\"   - Measures: Ranking quality of retrieved chunks\")\n",
    "print(\"   - Type: LLM-based ground truth comparison\")\n",
    "print(\"   - Initialization: ContextPrecision(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1s per question\")\n",
    "print(\"   - Why: Evaluates retrieval algorithm effectiveness\")\n",
    "\n",
    "print(\"\\n4. CONTEXT RECALL\")\n",
    "print(\"   - Measures: Completeness of retrieved information\")\n",
    "print(\"   - Type: LLM-based ground truth comparison\")\n",
    "print(\"   - Initialization: ContextRecall(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1s per question\")\n",
    "print(\"   - Why: Ensures no critical information is missed\")\n",
    "\n",
    "print(\"\\n5. CONTEXT ENTITY RECALL\")\n",
    "print(\"   - Measures: Key entities from ground truth present in retrieved contexts\")\n",
    "print(\"   - Type: LLM-based entity extraction and matching\")\n",
    "print(\"   - Initialization: ContextEntityRecall(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.6\")\n",
    "print(\"   - Speed: ~1-2s per question\")\n",
    "print(\"   - Why: Ensures important named entities are retrieved\")\n",
    "\n",
    "print(\"\\n6. NOISE SENSITIVITY\")\n",
    "print(\"   - Measures: Robustness to irrelevant/noisy context\")\n",
    "print(\"   - Type: LLM-based evaluation with added noise\")\n",
    "print(\"   - Initialization: NoiseSensitivity(llm=ragas_llm)\")\n",
    "print(\"   - Target: >0.7\")\n",
    "print(\"   - Speed: ~2-3s per question\")\n",
    "print(\"   - Why: Tests if system can ignore distractors\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Œ Note on Ragas 0.3.9:\")\n",
    "print(\"   All 6 non-multimodal metrics are now configured.\")\n",
    "print(\"   Metrics are initialized as classes with wrapped LLM/embeddings.\")\n",
    "print(\"   LangChain components must be wrapped using Ragas wrapper classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc34edfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ragas evaluation...\n",
      "  Dataset: 18 questions\n",
      "  Metrics: 6 (all 6 non-multimodal metrics)\n",
      "  Estimated time: 7-12 minutes\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288963573ff74a8bae878e2e0bf9c994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[5]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[23]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[47]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[53]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[59]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[65]: TimeoutError()\n",
      "Exception raised in Job[71]: TimeoutError()\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[77]: TimeoutError()\n",
      "Exception raised in Job[83]: TimeoutError()\n",
      "Exception raised in Job[89]: TimeoutError()\n",
      "Exception raised in Job[95]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Evaluation completed!\n",
      "\n",
      "================================================================================\n",
      "RAGAS EVALUATION RESULTS (6 Metrics)\n",
      "================================================================================\n",
      "{'faithfulness': 0.9768, 'answer_relevancy': 0.8739, 'context_precision': 0.7870, 'context_recall': 0.9815, 'context_entity_recall': 0.4850, 'noise_sensitivity(mode=relevant)': 0.5000}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Ragas evaluation...\")\n",
    "print(f\"  Dataset: {len(eval_dataset)} questions\")\n",
    "print(f\"  Metrics: {len(metrics)} (all 6 non-multimodal metrics)\")\n",
    "print(f\"  Estimated time: 7-12 minutes\\n\")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation completed!\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAGAS EVALUATION RESULTS (6 Metrics)\")\n",
    "print(\"=\" * 80)\n",
    "print(result)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dc1cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Results by Question:\n",
      "\n",
      "                                                                            user_input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     retrieved_contexts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       response                                                                                                                                                                                                                                                         reference  faithfulness  answer_relevancy  context_precision  context_recall  context_entity_recall  noise_sensitivity(mode=relevant)\n",
      "0                                                      What is CloudFlow's uptime SLA?                   [CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, CloudFlow Support Escalation Process\\n\\nUnderstanding CloudFlow's support escalation process ensures your issues are resolved efficiently.\\n\\nSupport Channels:\\n- Community Forums (All tiers): community.cloudflow.io - Best for general questions, feature requests, and sharing knowledge\\n- Email Support (Premium & Enterprise): support@cloudflow.io - Include account ID and request ID in subject line\\n- Phone Support (Enterprise only): +1-888-CLOUDFLOW - Available 24/7 for critical issues\\n- Slack Channel (Enterprise only): Direct access to engineering team\\n\\nIssue Severity Levels:\\n- P0 (Critical): Complete service outage affecting production. Response time: 1 hour for Enterprise, 4 hours for Premium\\n- P1 (High): Major functionality impaired but workarounds available. Response time: 4 hours for Enterprise, 8 hours for Premium\\n- P2 (Medium): Minor functionality issues with workarounds. Response time: 24 hours\\n- P3 (Low): Questions, feature requests, documentation issues. Response time: 48 hours]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         CloudFlow's uptime SLA is 99.99% for the Enterprise tier and 99.95% for the Premium tier. The Standard tier offers a 99.9% uptime SLA.                                                                                                                                                                          CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones.      1.000000          0.968056           1.000000        1.000000               0.750000                               NaN\n",
      "1                                     What authentication protocol does CloudFlow use?                                                                                                                                                                                                                                                                                                                                           [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     CloudFlow uses OAuth 2.0 and API Keys as authentication protocols. OAuth 2.0 is recommended for user-facing applications, while API Keys are ideal for server-to-server communication and background jobs.                                                                                                                                                            CloudFlow uses OAuth 2.0 for user-facing applications and API keys for server-to-server communication.      1.000000          0.951862           1.000000        1.000000               0.500000                               NaN\n",
      "2                               What is the service mesh technology used by CloudFlow?                                                                                                                                                                                                                                                 [CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The service mesh technology used by CloudFlow is Istio.                                                                                                                                                           CloudFlow uses Istio as the service mesh technology to orchestrate communication between microservices.      1.000000          0.995091           1.000000        1.000000               0.750000                               NaN\n",
      "3                                    What compliance standards does CloudFlow support?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits., PCI DSS: For customers processing payment card data, CloudFlow provides PCI DSS Level 1 certified infrastructure. However, we recommend using dedicated payment processors rather than storing card data.\\n\\nData Residency: CloudFlow supports data residency in US, EU, UK, and APAC regions to meet local regulatory requirements., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            CloudFlow supports the following compliance standards:\\n\\n1. **SOC 2 Type II**: Certified for security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n2. **GDPR Compliance**: Fully compliant with the European Union's General Data Protection Regulation, supporting data residency, right to erasure, data portability, and providing Data Processing Agreements (DPA).\\n3. **HIPAA**: Offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA), enhanced audit logging, encrypted backups, and strict access controls for healthcare customers.\\n4. **ISO 27001**: Certified to ISO 27001:2013 standards for information security management, with annual recertification audits.\\n5. **PCI DSS**: Provides PCI DSS Level 1 certified infrastructure for customers processing payment card data, though recommends using dedicated payment processors.\\n\\nAdditionally, CloudFlow supports data residency in the US, EU, UK, and APAC regions to meet local regulatory requirements.                                                                                                                                                               CloudFlow supports SOC 2 Type II, GDPR, HIPAA, ISO 27001, and PCI DSS Level 1 compliance standards.      1.000000          0.946637           0.000000        1.000000               0.833333                               NaN\n",
      "4                                          How long are CloudFlow audit logs retained?                                                                                                                                                                                                                                                                                                                                                                                                                        [Log Management: CloudFlow retains logs for 7 days by default (30 days for Premium, 90 days for Enterprise). Use structured logging with JSON format for better searchability. Maximum log line length: 32KB.\\n\\nDashboard Widgets: Create custom dashboards with real-time metrics, SLA compliance, and cost tracking. Share dashboards with team members or embed in external tools using iframe integration., Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\\n\\nVulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours., CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard., CloudFlow Compliance Standards\\n\\nCloudFlow maintains compliance with major industry standards and regulations to ensure your data is handled responsibly.\\n\\nSOC 2 Type II: CloudFlow is SOC 2 Type II certified, demonstrating our commitment to security, availability, and confidentiality. Audit reports are available to enterprise customers under NDA.\\n\\nGDPR Compliance: CloudFlow is fully compliant with the European Union's General Data Protection Regulation. We support data residency requirements, right to erasure, data portability, and provide Data Processing Agreements (DPA) to all customers.\\n\\nHIPAA: For healthcare customers, CloudFlow offers HIPAA-compliant infrastructure with Business Associate Agreements (BAA). HIPAA features include enhanced audit logging, encrypted backups, and strict access controls.\\n\\nISO 27001: CloudFlow's information security management system is certified to ISO 27001:2013 standards. We maintain comprehensive security policies and undergo annual recertification audits.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 CloudFlow audit logs are retained for 2 years.                                                                                                                                                                                                      CloudFlow audit logs are immutable and retained for 2 years.      1.000000          0.999998           0.500000        1.000000               0.666667                               NaN\n",
      "5                            What are the three main layers of CloudFlow architecture?                                                                                                                                                                                                                                                 [CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The three main layers of CloudFlow architecture are:\\n\\n1. **API Gateway Layer**: Handles incoming requests using OAuth 2.0 authentication, provides load balancing, SSL termination, and request routing capabilities, and routes requests through the service mesh powered by Istio.\\n\\n2. **Service Mesh Layer**: Orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\n3. **Data Storage Layer**: Implements a distributed database system with automatic replication across three availability zones, ensuring data durability and supporting horizontal scaling based on demand.                                                               The three main layers are: API Gateway layer (handles authentication and routing), Service Mesh layer (orchestrates microservices), and Data Storage layer (distributed database with replication).      1.000000          0.999877           1.000000        1.000000               0.500000                               NaN\n",
      "6                        What are CloudFlow's pricing tiers and their API rate limits?                                                              [CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., CloudFlow Billing Information\\n\\nUnderstanding CloudFlow's billing model helps you manage costs effectively and avoid unexpected charges.\\n\\nBilling Cycle: Subscriptions are billed monthly on the date you signed up. Annual subscriptions are billed upfront with a 15% discount. Billing date can be changed once per year.\\n\\nUsage-Based Charges: Beyond included quotas, additional usage is billed at:\\n- API requests: $0.01 per 1,000 requests\\n- Storage: $0.10 per GB per month\\n- Bandwidth: $0.08 per GB\\n- Backup retention (beyond 30 days): $0.05 per GB per month\\n\\nPayment Methods: CloudFlow accepts credit cards (Visa, Mastercard, Amex), ACH transfers (US only), and wire transfers for invoices over $1,000. Cryptocurrency payments available for annual plans.\\n\\nInvoicing: Invoices are emailed on the billing date and available in the dashboard. Enterprise customers receive consolidated monthly invoices with 30-day payment terms., API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         CloudFlow offers three pricing tiers with the following API rate limits:\\n\\n1. **Standard Tier ($99/month)**:\\n   - API rate limit: 1,000 requests per hour per API key\\n   - Burst capacity: Up to 100 requests per minute\\n\\n2. **Premium Tier ($499/month)**:\\n   - API rate limit: 10,000 requests per hour per API key\\n   - Burst capacity: Up to 500 requests per minute\\n   - Includes priority request processing\\n\\n3. **Enterprise Tier (Custom pricing)**:\\n   - API rate limit: Custom, typically starting at 100,000 requests per hour\\n   - Dedicated infrastructure for custom rate limits\\n\\nRate limit headers are included in API responses, and exceeding limits returns an HTTP 429 (Too Many Requests) response.                                                                              Standard tier costs $99/month with 1,000 requests/hour, Premium tier costs $499/month with 10,000 requests/hour, and Enterprise tier has custom pricing with 100,000+ requests/hour.      1.000000          0.983564           1.000000        1.000000               0.333333                               NaN\n",
      "7            What HTTP status codes indicate authentication failures in CloudFlow API?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header., Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}, Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1).]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The HTTP status codes that indicate authentication failures in the CloudFlow API are:\\n\\n- **401 Unauthorized**: Missing or invalid API key. Check the Authorization header.\\n- **403 Forbidden**: Valid API key but insufficient permissions for the requested operation.                                                                                                                                    401 Unauthorized indicates missing or invalid API key, and 403 Forbidden indicates valid API key but insufficient permissions.      1.000000          0.993424           1.000000        1.000000               0.750000                               NaN\n",
      "8                                    What auto-scaling metrics does CloudFlow monitor?                                                                                                                                                    [CloudFlow Scaling Mechanisms\\n\\nCloudFlow implements sophisticated auto-scaling mechanisms to handle varying workloads efficiently. The platform monitors real-time metrics to make intelligent scaling decisions.\\n\\nHorizontal Pod Autoscaling (HPA) adjusts the number of pod replicas based on CPU utilization (target: 70%) and memory usage (target: 80%). When these thresholds are exceeded for more than 3 consecutive minutes, the system automatically provisions additional pods.\\n\\nVertical scaling adjusts resource allocation for individual services. CloudFlow can increase or decrease CPU and memory limits without downtime, using Kubernetes resource management capabilities.\\n\\nThe platform supports bursting to handle sudden traffic spikes. During burst periods, CloudFlow can temporarily scale up to 500% of baseline capacity for up to 15 minutes before triggering permanent scaling., CloudFlow Monitoring and Observability\\n\\nEffective monitoring ensures your CloudFlow applications remain healthy and performant.\\n\\nMetrics Collection: CloudFlow automatically collects key metrics including request rate, error rate, latency (p50, p95, p99), and resource utilization. Access metrics via the dashboard or Metrics API at /api/v1/metrics.\\n\\nCustom Metrics: Send custom application metrics using the StatsD protocol. CloudFlow aggregates custom metrics every 60 seconds and retains them for 90 days.\\n\\nAlerting: Configure alerts for critical conditions like error rate >5%, latency >500ms, or approaching rate limits. CloudFlow supports alerting via email, SMS, Slack, PagerDuty, and webhooks.\\n\\nDistributed Tracing: Enable distributed tracing to track requests across services. CloudFlow supports OpenTelemetry and provides trace visualization in the dashboard. Sample rate: 10% of requests (configurable up to 100%)., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow System Components\\n\\nCloudFlow's architecture comprises several key components that work in harmony to deliver reliable cloud services.\\n\\nThe Control Plane manages the overall system state, including service registration, configuration management, and orchestration. It runs on a dedicated cluster with five replicas for high availability.\\n\\nThe Data Plane handles actual request processing and data flow. It consists of worker nodes that execute application workloads and process user requests. Each data plane node has 16 CPU cores and 64GB RAM.\\n\\nThe Observability Stack includes Prometheus for metrics collection, Grafana for visualization, and ELK (Elasticsearch, Logstash, Kibana) for log aggregation. Metrics are collected every 15 seconds and retained for 90 days.\\n\\nThe Service Registry maintains a real-time directory of all available services and their endpoints. It uses etcd for distributed consensus and supports automatic service discovery with DNS-based lookups.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   CloudFlow monitors CPU utilization (target: 70%) and memory usage (target: 80%) for auto-scaling decisions. These metrics are used by the Horizontal Pod Autoscaling (HPA) mechanism to adjust the number of pod replicas. Additionally, the platform monitors real-time metrics to handle bursting and scaling efficiently.                                                                                                CloudFlow monitors CPU utilization (target 70%), memory usage (target 80%), and triggers scaling when thresholds are exceeded for more than 3 consecutive minutes.      0.800000          0.972920           1.000000        1.000000               0.500000                               NaN\n",
      "9                          How do I authenticate with CloudFlow APIs using an API key?                                                                                                                                                                                                                                                                                                                                                                  [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint., Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1)., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              To authenticate with CloudFlow APIs using an API key, include your API key in the Authorization header of your request. The format should be:\\n\\n```\\nAuthorization: Bearer YOUR_API_KEY\\n```\\n\\nEnsure the API key starts with \"cf_live_\" followed by 32 alphanumeric characters, making it exactly 40 characters long. All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error. Additionally, API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the `/api/v1/keys` endpoint.                                                   Include your API key in the Authorization header as 'Authorization: Bearer YOUR_API_KEY'. All requests must be made over HTTPS, and API keys have the format 'cf_live_' followed by 32 alphanumeric characters.      1.000000          0.994559           1.000000        1.000000               0.400000                               NaN\n",
      "10                                     How do I handle rate limit errors in CloudFlow?  [Common CloudFlow Errors and Solutions\\n\\nThis guide covers the most common errors encountered when using CloudFlow and their solutions.\\n\\nError: \"Invalid API Key\" (401)\\nSolution: Verify your API key format starts with \"cf_live_\" and is exactly 40 characters. Check for extra spaces or newlines. Generate a new API key if the issue persists. API keys are case-sensitive.\\n\\nError: \"Rate Limit Exceeded\" (429)\\nSolution: Implement exponential backoff in your retry logic. Check X-RateLimit-Reset header to know when limits reset. Consider upgrading to a higher tier if you consistently hit limits. Use batch endpoints to reduce request count.\\n\\nError: \"Resource Not Found\" (404)\\nSolution: Verify the resource ID is correct and the resource hasn't been deleted. Use the /api/v1/resources endpoint to list available resources. Check if you're using the correct API version (/v1)., CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc., Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., CloudFlow API Error Codes\\n\\nCloudFlow APIs use standard HTTP status codes and provide detailed error messages in JSON format to help diagnose issues.\\n\\nAuthentication Errors:\\n- 401 Unauthorized: Missing or invalid API key. Check the Authorization header.\\n- 403 Forbidden: Valid API key but insufficient permissions for the requested operation.\\n\\nClient Errors:\\n- 400 Bad Request: Invalid request format or missing required fields. The response includes a \"details\" field explaining what's wrong.\\n- 404 Not Found: Requested resource doesn't exist. Verify the resource ID.\\n- 409 Conflict: Request conflicts with current resource state (e.g., duplicate name).\\n- 422 Unprocessable Entity: Request format is valid but contains semantic errors.\\n- 429 Too Many Requests: Rate limit exceeded. Check X-RateLimit-Reset header.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   To handle rate limit errors (HTTP 429) in CloudFlow:\\n\\n1. **Implement Exponential Backoff**: Use a retry logic that waits progressively longer between retries (e.g., wait 1 second, then 2 seconds, then 4 seconds, etc.).\\n\\n2. **Check Rate Limit Headers**:\\n   - **X-RateLimit-Reset**: Indicates the Unix timestamp when the rate limit resets. Use this to determine when you can resume sending requests.\\n   - **Retry-After**: Specifies how many seconds to wait before retrying.\\n\\n3. **Optimize API Usage**:\\n   - Use batch endpoints to reduce the number of individual requests.\\n   - Consider upgrading to a higher tier (Premium or Enterprise) if you consistently hit rate limits.\\n\\n4. **Understand Tier Limits**:\\n   - **Standard Tier**: 1,000 requests per hour, with a burst capacity of 100 requests per minute.\\n   - **Premium Tier**: 10,000 requests per hour, with a burst capacity of 500 requests per minute.\\n   - **Enterprise Tier**: Custom limits starting at 100,000 requests per hour.\\n\\nBy following these steps, you can effectively manage rate limit errors and ensure smoother API usage.                                     When you receive a 429 error, implement exponential backoff in retry logic, check the X-RateLimit-Reset header to know when limits reset, and use the Retry-After header to determine wait time (1s, then 2s, then 4s, etc.).      1.000000          0.961593           0.916667        1.000000               0.000000                               NaN\n",
      "11                   What steps should I follow to optimize CloudFlow API performance?                                                                                                                                                                                                                                                                                                                                                                                  [CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally., Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise., CloudFlow Debugging Guide\\n\\nWhen troubleshooting issues with CloudFlow, follow this systematic debugging approach.\\n\\nStep 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\\n\\nStep 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\\n\\nStep 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\\n\\nStep 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources, Asynchronous Processing: For long-running operations, use CloudFlow's async API endpoints. Poll for results using the returned job_id rather than blocking on the initial request.\\n\\nCDN Usage: Serve static assets through CloudFlow's global CDN with 150+ edge locations. Configure appropriate cache headers for optimal performance: max-age=3600 for semi-static content.]                            To optimize CloudFlow API performance, follow these steps:\\n\\n1. **Caching Strategy**: \\n   - Use CloudFlow's built-in Redis cache for frequently accessed data with a TTL of 5-60 minutes.\\n   - Cache API responses on the client side and respect Cache-Control headers.\\n\\n2. **Request Optimization**: \\n   - Batch multiple operations into single API calls when possible.\\n   - Use pagination for large result sets with a recommended page size of 50-100 items.\\n   - Implement request compression using gzip to reduce bandwidth usage.\\n\\n3. **Connection Management**: \\n   - Reuse HTTP connections with keep-alive headers.\\n   - Maintain a connection pool with 5-10 concurrent connections per API key.\\n   - Set appropriate timeouts: connection timeout of 10 seconds and read timeout of 30 seconds.\\n\\n4. **Query Efficiency**: \\n   - Use field filtering to request only the required data (e.g., `/resources?fields=id,name,status`).\\n   - Leverage server-side filtering instead of retrieving all data and filtering locally.\\n\\n5. **Network Connectivity**: \\n   - Ensure your network allows outbound HTTPS traffic to `*.cloudflow.io` on port 443.\\n   - Verify DNS resolution is working correctly.\\n\\n6. **SDK Version**: \\n   - Ensure you are using the latest version of the CloudFlow SDK to avoid compatibility issues and bugs.\\n\\n7. **Contact Support**: \\n   - If issues persist, contact CloudFlow support with the request ID from failed requests. Response times are 24 hours for Standard tier, 4 hours for Premium, and 1 hour for Enterprise.     Implement caching with Redis (TTL 5-60 minutes), batch multiple operations into single API calls, use pagination for large result sets (50-100 items), enable request compression with gzip, and maintain a connection pool with 5-10 concurrent connections.      1.000000          0.978115           1.000000        1.000000               0.555556                               NaN\n",
      "12                What's the difference between Standard and Premium tier rate limits?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       [OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier., Upgrades and Downgrades: Upgrade anytime to immediately access higher tier features. Downgrades take effect at the next billing cycle. Prorated credits are applied to your account balance.\\n\\nFree Trial: New customers get 14-day free trial on Premium tier with no credit card required. Trial includes 1,000 API requests and 10 GB storage., CloudFlow Pricing Tiers\\n\\nCloudFlow offers three pricing tiers designed to meet the needs of individuals, teams, and enterprises.\\n\\nStandard Tier ($99/month):\\n- 1,000 API requests per hour\\n- 100 GB storage included\\n- 10 GB bandwidth per month\\n- Community support via forums\\n- 99.9% uptime SLA\\n- Up to 5 team members\\n\\nPremium Tier ($499/month):\\n- 10,000 API requests per hour\\n- 1 TB storage included\\n- 100 GB bandwidth per month\\n- Email support with 24-hour response time\\n- 99.95% uptime SLA\\n- Up to 25 team members\\n- Advanced monitoring and alerting\\n- Custom domain support\\n\\nEnterprise Tier (Custom pricing):\\n- Custom API rate limits (100,000+ requests/hour)\\n- Unlimited storage and bandwidth\\n- 24/7 phone and email support with 1-hour response time\\n- 99.99% uptime SLA with service credits\\n- Unlimited team members\\n- Dedicated account manager\\n- Custom integrations and professional services\\n- Private cloud deployment options, CloudFlow API Rate Limiting\\n\\nCloudFlow implements rate limiting to ensure fair usage and platform stability. Rate limits vary by pricing tier and authentication method.\\n\\nStandard Tier: 1,000 requests per hour per API key. Burst capacity allows up to 100 requests per minute. Exceeding limits returns HTTP 429 (Too Many Requests).\\n\\nPremium Tier: 10,000 requests per hour per API key with burst capacity of 500 requests per minute. Premium tier also includes priority request processing.\\n\\nEnterprise Tier: Custom rate limits negotiated based on usage patterns. Typically starts at 100,000 requests per hour with dedicated infrastructure.\\n\\nRate limit headers are included in every response:\\n- X-RateLimit-Limit: Maximum requests per hour\\n- X-RateLimit-Remaining: Remaining requests in current window\\n- X-RateLimit-Reset: Unix timestamp when the limit resets\\n\\nWhen rate limited, the Retry-After header indicates how many seconds to wait before retrying. Implement exponential backoff: wait 1s, then 2s, then 4s, etc.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The difference between Standard and Premium tier rate limits is as follows:\\n\\n- **Standard Tier**: Allows 1,000 API requests per hour per API key, with a burst capacity of up to 100 requests per minute.\\n- **Premium Tier**: Allows 10,000 API requests per hour per API key, with a burst capacity of up to 500 requests per minute. Additionally, Premium tier includes priority request processing.                                        Standard tier allows 1,000 requests per hour with 100 requests per minute burst, while Premium tier allows 10,000 requests per hour with 500 requests per minute burst. Premium also includes priority request processing.      1.000000          0.996236           0.250000        1.000000               0.571429                               NaN\n",
      "13  How does OAuth 2.0 authentication differ from API key authentication in CloudFlow?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         [CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security. You can manage your API keys through the CloudFlow dashboard or the /api/v1/keys endpoint., OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier., CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect.]                                                                                                                                           OAuth 2.0 authentication differs from API key authentication in CloudFlow in the following ways:\\n\\n1. **Use Case**:  \\n   - OAuth 2.0 is recommended for user-facing applications, as it involves user authorization and provides access tokens and refresh tokens.  \\n   - API keys are ideal for server-to-server communication and background jobs, as they do not require user interaction.\\n\\n2. **Token Expiry**:  \\n   - OAuth 2.0 access tokens are valid for 1 hour, and refresh tokens are valid for 30 days.  \\n   - API keys do not expire unless explicitly revoked.\\n\\n3. **Authentication Method**:  \\n   - OAuth 2.0 requires directing users to an authorization endpoint (`https://auth.cloudflow.io/oauth/authorize`) with `client_id` and `redirect_uri` parameters to obtain tokens.  \\n   - API keys are included directly in the Authorization header (`Authorization: Bearer YOUR_API_KEY`) for requests.\\n\\n4. **Security Features**:  \\n   - API keys can be scoped to specific permissions (read, write, admin) and restricted to specific IP addresses for enhanced security.  \\n   - OAuth 2.0 provides higher rate limits for authenticated requests (5,000 requests per hour for Standard tier).\\n\\n5. **Expiration and Management**:  \\n   - OAuth 2.0 tokens require periodic renewal using refresh tokens.  \\n   - API keys are managed through the CloudFlow dashboard or the `/api/v1/keys` endpoint and do not require renewal unless revoked.  OAuth 2.0 is recommended for user-facing applications with access tokens valid for 1 hour and provides the Authorization Code flow, while API keys are ideal for server-to-server communication, never expire unless revoked, and have a simpler implementation.      1.000000          1.000000           1.000000        1.000000               0.500000                               NaN\n",
      "14                                  What should I do if I receive a 504 timeout error?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             [Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format., Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise., Server Errors:\\n- 500 Internal Server Error: Unexpected server error. CloudFlow team is automatically notified.\\n- 502 Bad Gateway: Temporary issue with upstream services. Retry after a few seconds.\\n- 503 Service Unavailable: Scheduled maintenance or system overload. Check status.cloudflow.io.\\n\\nError Response Format: {\"error\": {\"code\": \"error_code\", \"message\": \"Human-readable message\", \"details\": {...}}}, OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           If you receive a 504 timeout error, you should:\\n\\n1. Increase the client timeout to at least 30 seconds.\\n2. For long-running operations, use asynchronous (async) endpoints and poll for results.\\n3. Check the CloudFlow status page for any service degradation.                                                                            Increase client timeout to at least 30 seconds, use async endpoints for long-running operations and poll for results, and check the CloudFlow status page for any service degradation.      1.000000          0.989240           1.000000        1.000000               0.500000                               NaN\n",
      "15                                How do I debug slow API response times in CloudFlow?                                                                                                                                                                                                                                                                                 [CloudFlow Debugging Guide\\n\\nWhen troubleshooting issues with CloudFlow, follow this systematic debugging approach.\\n\\nStep 1 - Check Service Status: Visit status.cloudflow.io to verify all systems are operational. Subscribe to status updates to receive notifications about incidents and maintenance.\\n\\nStep 2 - Review API Logs: Access detailed API logs in the CloudFlow dashboard under Analytics > API Logs. Filter by time range, status code, and endpoint. Look for patterns in failed requests.\\n\\nStep 3 - Enable Debug Mode: Add X-CloudFlow-Debug: true header to requests to receive detailed debug information in responses. Debug mode provides request ID, processing time breakdown, and backend service information.\\n\\nStep 4 - Test with curl: Isolate issues by testing with curl commands. Example: curl -H \"Authorization: Bearer YOUR_API_KEY\" -H \"X-CloudFlow-Debug: true\" https://api.cloudflow.io/v1/resources, Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise., CloudFlow Performance Optimization\\n\\nFollowing these best practices will help you achieve optimal performance from your CloudFlow applications.\\n\\nCaching Strategy: Implement caching at multiple levels. Use CloudFlow's built-in Redis cache for frequently accessed data with TTL between 5-60 minutes. Cache API responses on the client side and respect Cache-Control headers.\\n\\nRequest Optimization: Batch multiple operations into single API calls when possible. Use pagination for large result sets (recommended page size: 50-100 items). Implement request compression using gzip to reduce bandwidth.\\n\\nConnection Management: Reuse HTTP connections with keep-alive headers. Maintain a connection pool with 5-10 concurrent connections per API key. Set appropriate timeouts: connection timeout 10s, read timeout 30s.\\n\\nQuery Efficiency: Use field filtering to request only required data: /resources?fields=id,name,status. Leverage server-side filtering instead of retrieving all data and filtering locally., Error: \"Timeout\" (504)\\nSolution: Increase client timeout to at least 30 seconds. For long-running operations, use async endpoints and poll for results. Check CloudFlow status page for any service degradation.\\n\\nError: \"Validation Error\" (422)\\nSolution: Review the error details field for specific validation failures. Common issues: missing required fields, invalid data types, values outside allowed ranges. Consult API documentation for correct request format.]  To debug slow API response times in CloudFlow, follow these steps:\\n\\n1. **Enable Debug Mode**: Add the header `X-CloudFlow-Debug: true` to your API requests. This will provide detailed debug information in the response, including request ID, processing time breakdown, and backend service details.\\n\\n2. **Review API Logs**: Access the API logs in the CloudFlow dashboard under **Analytics > API Logs**. Filter logs by time range, endpoint, and status code to identify patterns or specific endpoints with slow responses.\\n\\n3. **Check Network Connectivity**: Ensure your network allows outbound HTTPS traffic to `*.cloudflow.io` on port 443 and verify DNS resolution is functioning correctly.\\n\\n4. **Optimize Requests**: Implement request optimization techniques such as batching operations, using pagination (recommended page size: 50-100 items), and enabling gzip compression to reduce bandwidth.\\n\\n5. **Connection Management**: Reuse HTTP connections with keep-alive headers, maintain a connection pool with 5-10 concurrent connections per API key, and set appropriate timeouts (connection timeout: 10 seconds, read timeout: 30 seconds).\\n\\n6. **Check Service Status**: Visit [status.cloudflow.io](https://status.cloudflow.io) to ensure there are no ongoing incidents or service degradation affecting response times.\\n\\n7. **Contact Support**: If the issue persists, contact CloudFlow support with the request ID from the debug information. Response times vary based on your support tier: 24 hours for Standard, 4 hours for Premium, and 1 hour for Enterprise.                               Add X-CloudFlow-Debug: true header to requests for detailed debug information, review API logs in the dashboard under Analytics > API Logs, test with curl commands, and verify network connectivity to *.cloudflow.io on port 443.      0.782609          0.998481           1.000000        1.000000               0.285714                               NaN\n",
      "16                                       What happens if I use an expired OAuth token?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [OAuth 2.0 authenticated requests have separate, higher limits: 5,000 requests per hour for Standard tier., CloudFlow API Authentication\\n\\nCloudFlow APIs support two authentication methods: OAuth 2.0 and API Keys. Both methods provide secure access to platform resources.\\n\\nOAuth 2.0 is recommended for user-facing applications. It supports the Authorization Code flow and provides access tokens valid for 1 hour and refresh tokens valid for 30 days. To implement OAuth 2.0, direct users to the authorization endpoint at https://auth.cloudflow.io/oauth/authorize with your client_id and redirect_uri parameters.\\n\\nAPI Keys are ideal for server-to-server communication and background jobs. Each API key has the format \"cf_live_\" followed by 32 alphanumeric characters. API keys never expire unless explicitly revoked.\\n\\nTo authenticate requests, include your API key in the Authorization header: \"Authorization: Bearer YOUR_API_KEY\". All API requests must be made over HTTPS; HTTP requests will be rejected with a 403 error., Audit Logging: Every API call is logged with timestamp, user identity, IP address, and action taken. Audit logs are immutable and retained for 2 years. You can access logs via the /api/v1/audit-logs endpoint.\\n\\nVulnerability Management: CloudFlow undergoes quarterly penetration testing by independent security firms. We maintain a bug bounty program and respond to security reports within 24 hours., Step 5 - Check Network Connectivity: Ensure your network allows outbound HTTPS traffic to *.cloudflow.io on port 443. Verify DNS resolution is working correctly.\\n\\nStep 6 - Verify SDK Version: If using CloudFlow SDK, ensure you're running the latest version. Outdated SDKs may not support new API features or may have known bugs.\\n\\nStep 7 - Contact Support: If issues persist, contact CloudFlow support with the request ID from failed requests. Support responds within 24 hours for Standard tier, 4 hours for Premium, 1 hour for Enterprise.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I don't have enough information to answer that question.                                         If you use an expired access token, you'll receive a 401 Unauthorized error. You should use your refresh token to obtain a new access token. Access tokens are valid for 1 hour and refresh tokens are valid for 30 days.      1.000000          0.000000           0.500000        0.666667               0.333333                               1.0\n",
      "17                                      Does CloudFlow support blockchain integration?                                                                                                                                                                                                                                                              [CloudFlow Security Features\\n\\nSecurity is a top priority at CloudFlow. We implement industry-leading security practices to protect your data and applications.\\n\\nEncryption: All data is encrypted at rest using AES-256 encryption. Data in transit uses TLS 1.3 with perfect forward secrecy. Encryption keys are rotated every 90 days using AWS KMS.\\n\\nNetwork Security: CloudFlow runs in a Virtual Private Cloud (VPC) with strict network segmentation. Public endpoints are protected by Web Application Firewall (WAF) rules that block common attack patterns. DDoS protection is provided by Cloudflare with mitigation capacity up to 50 Gbps.\\n\\nAccess Control: All resources support Role-Based Access Control (RBAC) with customizable roles and permissions. We support integration with external identity providers via SAML 2.0 and OpenID Connect., CloudFlow guarantees 99.99% uptime SLA with triple redundancy across availability zones. The platform supports horizontal scaling with automatic load balancing, allowing each service to scale independently based on CPU and memory metrics., CloudFlow Architecture Overview\\n\\nCloudFlow is a distributed cloud platform built on microservices architecture. The platform consists of three main layers that work together to provide a robust, scalable infrastructure.\\n\\nThe API Gateway layer handles all incoming requests using OAuth 2.0 authentication and routes them through our service mesh powered by Istio. This layer provides load balancing, SSL termination, and request routing capabilities.\\n\\nThe Service Mesh layer orchestrates communication between microservices, providing service discovery, health checking, and automatic failover. It uses Kubernetes for container orchestration across multiple availability zones.\\n\\nThe Data Storage layer implements a distributed database system with automatic replication across three availability zones. This ensures data durability and supports horizontal scaling based on demand., CloudFlow Disaster Recovery\\n\\nCloudFlow implements comprehensive disaster recovery capabilities to protect your data and ensure business continuity.\\n\\nBackup Strategy: CloudFlow performs automatic daily backups of all data at 2 AM UTC. Backups are encrypted and stored in geographically diverse locations. Retention: 30 days for Standard tier, 90 days for Premium, 1 year for Enterprise.\\n\\nPoint-in-Time Recovery: Enterprise customers can restore data to any point within the retention period with 5-minute granularity. Recovery operations typically complete within 15-30 minutes.\\n\\nMulti-Region Replication: Enable multi-region replication for critical data. Data is asynchronously replicated to a secondary region within 60 seconds. Failover to secondary region is automatic and takes approximately 5 minutes.\\n\\nBackup Verification: CloudFlow performs monthly backup restoration tests to ensure data recoverability. Test results are available in your compliance dashboard.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I don't have enough information to answer that question.                                                                                                                                                                                                          I don't have enough information to answer that question.      1.000000          0.000000           0.000000        1.000000               0.000000                               0.0\n"
     ]
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "\n",
    "print(\"Detailed Results by Question:\\n\")\n",
    "print(result_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bab68e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "       faithfulness  answer_relevancy  context_precision  context_recall  \\\n",
      "count     18.000000         18.000000          18.000000       18.000000   \n",
      "mean       0.976812          0.873870           0.787037        0.981481   \n",
      "std        0.067554          0.318361           0.364053        0.078567   \n",
      "min        0.782609          0.000000           0.000000        0.666667   \n",
      "25%        1.000000          0.963209           0.604167        1.000000   \n",
      "50%        1.000000          0.986402           1.000000        1.000000   \n",
      "75%        1.000000          0.995950           1.000000        1.000000   \n",
      "max        1.000000          1.000000           1.000000        1.000000   \n",
      "\n",
      "       context_entity_recall  noise_sensitivity(mode=relevant)  \n",
      "count              18.000000                          2.000000  \n",
      "mean                0.484965                          0.500000  \n",
      "std                 0.235395                          0.707107  \n",
      "min                 0.000000                          0.000000  \n",
      "25%                 0.350000                          0.250000  \n",
      "50%                 0.500000                          0.500000  \n",
      "75%                 0.642857                          0.750000  \n",
      "max                 0.833333                          1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(result_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e598f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
