{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fd63e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a00839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa53e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from ragas import SingleTurnSample, EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    LLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    NoiseSensitivity\n",
    ")\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a62607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: gpt-4o\n",
      "‚úÖ Embeddings initialized: text-embedding-ada-002\n",
      "‚úÖ RAGAS wrappers ready\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    ")\n",
    "\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "print(\"‚úÖ LLM initialized: gpt-4o\")\n",
    "print(\"‚úÖ Embeddings initialized: text-embedding-ada-002\")\n",
    "print(\"‚úÖ RAGAS wrappers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99b737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_async(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308bc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Response to evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.'\n",
      "\n",
      "üìö Retrieved context:\n",
      "   'The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.'\n"
     ]
    }
   ],
   "source": [
    "test_response = \"The first Super Bowl was held on January 15, 1967 in Los Angeles. It was a sunny day with clear skies.\"\n",
    "test_context = [\n",
    "    \"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles, California.\"\n",
    "]\n",
    "print(\"üìù Response to evaluate:\")\n",
    "print(f\"   '{test_response}'\")\n",
    "print(\"\\nüìö Retrieved context:\")\n",
    "print(f\"   '{test_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d13c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 1: Extracted Claims from Response\n",
      "==================================================\n",
      "1. The First AFL-NFL World Championship Game was played on January 15, 1967.  \n",
      "2. The game took place at the Los Angeles Memorial Coliseum.  \n",
      "3. The game was held in Los Angeles, California.\n"
     ]
    }
   ],
   "source": [
    "claim_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following response, extract ALL factual claims as a numbered list.\n",
    "Each claim should be a single, verifiable statement.\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Extract each factual claim:\n",
    "\"\"\")\n",
    "\n",
    "claim_chain = claim_extraction_prompt | llm | StrOutputParser()\n",
    "extracted_claim_raw = claim_chain.invoke({\"response\":test_context})\n",
    "\n",
    "print(\"üîç STEP 1: Extracted Claims from Response\")\n",
    "print(\"=\" * 50)\n",
    "print(extracted_claim_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a35556a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Claims to verify:\n",
      "   1. The first Super Bowl was held on January 15, 1967\n",
      "   2. The first Super Bowl was held in Los Angeles\n",
      "   3. It was a sunny day\n",
      "   4. There were clear skies\n"
     ]
    }
   ],
   "source": [
    "claims = [\n",
    "    \"The first Super Bowl was held on January 15, 1967\",\n",
    "    \"The first Super Bowl was held in Los Angeles\",\n",
    "    \"It was a sunny day\",\n",
    "    \"There were clear skies\"\n",
    "]\n",
    "print(\"üìã Claims to verify:\")\n",
    "for i, claim in enumerate(claims, 1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea2925a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 2: Verifying Each Claim Against Context\n",
      "============================================================\n",
      "\n",
      "‚úÖ Claims: The first Super Bowl was held on January 15, 1967\n",
      "    Result: **SUPPORTED**\n",
      "\n",
      "**Explanation:** The context states that the First AFL-NFL World Championship Game wa...\n",
      "\n",
      "‚úÖ Claims: The first Super Bowl was held in Los Angeles\n",
      "    Result: SUPPORTED\n",
      "\n",
      "Explanation: The context states that the First AFL-NFL World Championship Game (the event...\n",
      "\n",
      "‚ùå Claims: It was a sunny day\n",
      "    Result: **NOT SUPPORTED**\n",
      "\n",
      "**Explanation:** The context provides information about the date, location, and e...\n",
      "\n",
      "‚ùå Claims: There were clear skies\n",
      "    Result: NOT SUPPORTED\n",
      "\n",
      "Explanation: The context does not provide any information about the weather condition...\n"
     ]
    }
   ],
   "source": [
    "verification_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following context and claim, determine if the claim is SUPPORTED by the context.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer with:\n",
    "- \"SUPPORTED\" if the claim can be verified from the context\n",
    "- \"NOT SUPPORTED\" if the claim cannot be verified or contradicts the context\n",
    "\n",
    "Also provide a brief explanation.\n",
    "\n",
    "Verdict:\n",
    "\"\"\")\n",
    "\n",
    "verify_claims = verification_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"üîç STEP 2: Verifying Each Claim Against Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "verification_results = []\n",
    "for claim in claims:\n",
    "    result = verify_claims.invoke({\n",
    "        \"context\":test_context[0],\n",
    "        \"claim\":claim\n",
    "    })\n",
    "    is_supported = \"SUPPORTED\" in result.upper() and \"NOT SUPPORTED\" not in result.upper()\n",
    "    verification_results.append({\n",
    "        \"claim\":claim,\n",
    "        \"supported\": is_supported,\n",
    "        \"explanation\":result,\n",
    "    })\n",
    "    status = \"‚úÖ\" if is_supported else \"‚ùå\"\n",
    "    print(f\"\\n{status} Claims: {claim}\")\n",
    "    print(f\"    Result: {result[:100]}...\" if len(result)>100 else f\"    Result: {result[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57d0bb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Claim Verification Summary\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clain</th>\n",
       "      <th>Supported?</th>\n",
       "      <th>Reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The first Super Bowl was held on January 15, 1967</td>\n",
       "      <td>‚úÖ Yes</td>\n",
       "      <td>Found in context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The first Super Bowl was held in Los Angeles</td>\n",
       "      <td>‚úÖ Yes</td>\n",
       "      <td>Found in context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was a sunny day</td>\n",
       "      <td>‚ùå No</td>\n",
       "      <td>HALLUCINATION - Not in context!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There were clear skies</td>\n",
       "      <td>‚ùå No</td>\n",
       "      <td>HALLUCINATION - Not in context!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Clain  ...                           Reason\n",
       "0  The first Super Bowl was held on January 15, 1967  ...                 Found in context\n",
       "1       The first Super Bowl was held in Los Angeles  ...                 Found in context\n",
       "2                                 It was a sunny day  ...  HALLUCINATION - Not in context!\n",
       "3                             There were clear skies  ...  HALLUCINATION - Not in context!\n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nüìä Claim Verification Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_verification = pd.DataFrame([\n",
    "    {\n",
    "        \"Clain\": r['claim'],\n",
    "        \"Supported?\":\"‚úÖ Yes\" if r[\"supported\"] else \"‚ùå No\",\n",
    "        \"Reason\":\"Found in context\" if r['supported'] else \"HALLUCINATION - Not in context!\"\n",
    "\n",
    "    }\n",
    "    for r in verification_results\n",
    "])\n",
    "df_verification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa0ed3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ STEP 3: Calculate Faithfulness Score\n",
      "==================================================\n",
      "\n",
      "   Supported claims: 2\n",
      "   Total claims: 4\n",
      "\n",
      "   Formula: Faithfulness = 2 / 4\n",
      "\n",
      "   üìä Manual Faithfulness Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "supported_count = sum(1 for r in verification_results if r['supported'])\n",
    "total_claims = len(verification_results)\n",
    "\n",
    "manual_faithfulness = supported_count/total_claims\n",
    "\n",
    "print(\"üî¢ STEP 3: Calculate Faithfulness Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Supported claims: {supported_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: Faithfulness = {supported_count} / {total_claims}\")\n",
    "print(f\"\\n   üìä Manual Faithfulness Score: {manual_faithfulness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3abf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Faithfulness Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation:  0.50\n",
      "   RAGAS metric score:  0.50\n",
      "\n",
      "   Difference: 0.00\n"
     ]
    }
   ],
   "source": [
    "faithfulness_sample = SingleTurnSample(\n",
    "    user_input=\"When was the first Super Bowl?\",\n",
    "    response=test_response,\n",
    "    retrieved_contexts=test_context\n",
    ")\n",
    "faithfulness_metric = Faithfulness(llm=ragas_llm)\n",
    "ragas_faithfulness = run_async(faithfulness_metric.single_turn_ascore(faithfulness_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Faithfulness Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation:  {manual_faithfulness:.2f}\")\n",
    "print(f\"   RAGAS metric score:  {ragas_faithfulness:.2f}\")\n",
    "print(f\"\\n   Difference: {abs(manual_faithfulness - ragas_faithfulness):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f7f580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Faithfulness Comparison: Different Scenarios\n",
      "======================================================================\n",
      "\n",
      "Perfect Faithfulness (No hallucinations): \n",
      "   Response: The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial \n",
      "Score: 1.00\n",
      "\n",
      "Partial Faithfulness (Some hallucinations): \n",
      "   Response: The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 wi\n",
      "Score: 0.33\n",
      "\n",
      "Zero Faithfulness (Complete hallucination): \n",
      "   Response: The first Super Bowl was held in Miami in 1970 and attracted over 100,000 specta\n",
      "Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "faithfulness_examples = [\n",
    "    {\n",
    "        \"name\": \"Perfect Faithfulness (No hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was played on January 15, 1967 at the Los Angeles Memorial Coliseum.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partial Faithfulness (Some hallucinations)\",\n",
    "        \"response\": \"The first Super Bowl was on January 15, 1967. The Green Bay Packers won 35-10 with Bart Starr as MVP.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Zero Faithfulness (Complete hallucination)\",\n",
    "        \"response\": \"The first Super Bowl was held in Miami in 1970 and attracted over 100,000 spectators.\",\n",
    "        \"context\": [\"The First AFL-NFL World Championship Game was played on January 15, 1967, at the Los Angeles Memorial Coliseum.\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üìä Faithfulness Comparison: Different Scenarios\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for example in faithfulness_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=\"Tell me about the first Super Bowl\",\n",
    "        response=example[\"response\"],\n",
    "        retrieved_contexts=example['context']\n",
    "    )\n",
    "    score = run_async(faithfulness_metric.single_turn_ascore(sample))\n",
    "\n",
    "    print(f\"\\n{example['name']}: \")\n",
    "    print(f\"   Response: {example['response'][:80] if len(example['response'])> 80 else example['response']}\")\n",
    "    print(f\"Score: { score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f695a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Original Question:\n",
      "   'When was the first Super Bowl?'\n",
      "\n",
      "üìù Answer to Evaluate:\n",
      "   'The first Super Bowl was held on January 15, 1967'\n"
     ]
    }
   ],
   "source": [
    "original_question = \"When was the first Super Bowl?\"\n",
    "test_answer = \"The first Super Bowl was held on January 15, 1967\"\n",
    "\n",
    "print(\"üìù Original Question:\")\n",
    "print(f\"   '{original_question}'\")\n",
    "print(\"\\nüìù Answer to Evaluate:\")\n",
    "print(f\"   '{test_answer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc34bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 1: Generated Hypothetical Questions\n",
      "==================================================\n",
      "1. When was the first Super Bowl held?  \n",
      "2. What significant sports event took place on January 15, 1967?  \n",
      "3. Can you tell me the date of the inaugural Super Bowl?  \n"
     ]
    }
   ],
   "source": [
    "question_gen_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the following answer, generate exactly 3 different questions that this answer would be a good response to.\n",
    "The questions should be varied but all answerable by this response.\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Generate 3 questions (one per line):\n",
    "1.\n",
    "2.\n",
    "3.\n",
    "\"\"\")\n",
    "\n",
    "question_gen_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "generated_question = question_gen_chain.invoke({\"answer\":test_answer})\n",
    "\n",
    "print(\"üîç STEP 1: Generated Hypothetical Questions\")\n",
    "print(\"=\" * 50)\n",
    "print(generated_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "546ef65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Questions for embedding comparison:\n",
      "   Original:  When was the first Super Bowl?\n",
      "   Generated:\n",
      "      1. When was the first Super Bowl held?\n",
      "      2. What date was the inaugural Super Bowl?\n",
      "      3. On what day did the first Super Bowl take place?\n"
     ]
    }
   ],
   "source": [
    "generated_questions = [\n",
    "    \"When was the first Super Bowl held?\",\n",
    "    \"What date was the inaugural Super Bowl?\",\n",
    "    \"On what day did the first Super Bowl take place?\"\n",
    "]\n",
    "\n",
    "print(\"üìã Questions for embedding comparison:\")\n",
    "print(\"   Original: \",original_question)\n",
    "print(\"   Generated:\")\n",
    "for i,q in enumerate(generated_questions,1):\n",
    "    print(f\"      {i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5324863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cosine similarity function ready\n",
      "\n",
      "üìê Formula: cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "print(\"‚úÖ Cosine similarity function ready\")\n",
    "print(\"\\nüìê Formula: cos(Œ∏) = (A ¬∑ B) / (||A|| √ó ||B||)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76256cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP : Computing Embeddings and Similarities\n",
      "============================================================\n",
      "\n",
      "‚úÖ Original question embedded (dim=1536)\n",
      "\n",
      "   Question 1. 'When was the first Super Bowl held?'\n",
      "     Similarity to original: 0.9821\n",
      "\n",
      "   Question 2. 'What date was the inaugural Super Bowl?'\n",
      "     Similarity to original: 0.9415\n",
      "\n",
      "   Question 3. 'On what day did the first Super Bowl take place?'\n",
      "     Similarity to original: 0.9574\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç STEP : Computing Embeddings and Similarities\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "original_embedding = embeddings.embed_query(original_question)\n",
    "print(f\"\\n‚úÖ Original question embedded (dim={len(original_embedding)})\")\n",
    "\n",
    "similarities = []\n",
    "for i, gen_q in enumerate(generated_questions,1):\n",
    "    gen_embedding = embeddings.embed_query(gen_q)\n",
    "    sim = cosine_similarity(original_embedding, gen_embedding)\n",
    "    similarities.append(sim)\n",
    "\n",
    "    print(f\"\\n   Question {i}. '{gen_q}'\")\n",
    "    print(f\"     Similarity to original: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fbc12ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ STEP 3: Calculate Answer Relevancy Score\n",
      "==================================================\n",
      "   Similarities: ['0.9821', '0.9415', '0.9574']\n",
      "   Formula: Average of similarities\n",
      "\n",
      "  (0.9821+0.9415+0.9574)/3\n",
      "\n",
      "   üìä Manual Answer Relevancy: 0.9604\n"
     ]
    }
   ],
   "source": [
    "manual_relavancy = np.mean(similarities)\n",
    "print(\"üî¢ STEP 3: Calculate Answer Relevancy Score\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"   Similarities: {[f'{s:.4f}'for s in similarities]}\")\n",
    "\n",
    "print(f\"   Formula: Average of similarities\")\n",
    "print(f\"\\n  ({'+'.join([f'{s:.4f}' for s in similarities])})/{len(similarities)}\")\n",
    "print(f\"\\n   üìä Manual Answer Relevancy: {manual_relavancy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ccd75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Answer Relevancy Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation: 0.9604\n",
      "     Ragas metric score: 0.9821\n"
     ]
    }
   ],
   "source": [
    "relevancy_sample = SingleTurnSample(\n",
    "    user_input=original_question,\n",
    "    response=test_answer,\n",
    "    retrieved_contexts=[\"The First AFL-NFL World Championship Game was played on January 15, 1967.\"]\n",
    ")\n",
    "\n",
    "relevancy_metric = ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings)\n",
    "ragas_relevancy = run_async(relevancy_metric.single_turn_ascore(relevancy_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Answer Relevancy Result\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\" \\n    Manual calculation: {manual_relavancy:.4f}\")\n",
    "print(f\"     Ragas metric score: {ragas_relevancy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08b4e5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Comparison: \n",
      "======================================================================\n",
      "\n",
      "   Highly Relevant (Directly answers WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The first Super Bowl was held on January 15, 1967.'\n",
      "   Score: 0.9821\n",
      "\n",
      "   Partially Relevant (Answers but adds extra info)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the NFL, f...'\n",
      "   Score: 0.9455\n",
      "\n",
      "   Low Relevancy (Doesn't answer WHEN)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'The Super Bowl is the annual championship game of the Nation...'\n",
      "   Score: 0.8970\n",
      "\n",
      "   Off-topic (Completely irrelevant)\n",
      "   Q: 'When was the first Super Bowl?'\n",
      "   A: 'Pizza is a popular Italian dish that spread worldwide in the...'\n",
      "   Score: 0.7797\n"
     ]
    }
   ],
   "source": [
    "relevancy_examples = [\n",
    "    {\n",
    "        \"name\": \"Highly Relevant (Directly answers WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The first Super Bowl was held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partially Relevant (Answers but adds extra info)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the NFL, first held on January 15, 1967.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Low Relevancy (Doesn't answer WHEN)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"The Super Bowl is the annual championship game of the National Football League.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Off-topic (Completely irrelevant)\",\n",
    "        \"question\": \"When was the first Super Bowl?\",\n",
    "        \"answer\": \"Pizza is a popular Italian dish that spread worldwide in the 20th century.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Answer Relevancy Comparison: \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for example in relevancy_examples:\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=example['question'],\n",
    "        response=example['answer'],\n",
    "        retrieved_contexts=[\"Context not relevant for this metric.\"]\n",
    "    )\n",
    "    score = run_async(relevancy_metric.single_turn_ascore(sample))\n",
    "\n",
    "    print(f\"\\n   {example['name']}\")\n",
    "    print(f\"   Q: '{example['question']}'\")\n",
    "    print(f\"   A: '{example['answer'][:60]}...'\"  if len(example['answer'])>60 else f\"   A: '{example['answer']}'\")\n",
    "\n",
    "    print(f\"   Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ecf6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 'Where is the Eiffel Tower located?'\n",
      "\n",
      "Retrieved Chunks (with relevance):\n",
      "   1. ‚úÖ Relevant: The Eiffel Tower is located in Paris, France.\n",
      "   2. ‚úÖ Relevant: Paris is the capital of France.\n",
      "   3. ‚ùå Not relevant: The tower was built in 1889.\n",
      "   4. ‚ùå Not relevant: Pizza originated in Italy.\n"
     ]
    }
   ],
   "source": [
    "question = \"Where is the Eiffel Tower located?\"\n",
    "reference = \"The Eiffel Tower is located in Paris, France.\"\n",
    "\n",
    "chunks_with_relevance = [\n",
    "    (\"The Eiffel Tower is located in Paris, France.\", True),      # Directly relevant\n",
    "    (\"Paris is the capital of France.\", True),                     # Somewhat relevant\n",
    "    (\"The tower was built in 1889.\", False),                       # Not relevant to WHERE\n",
    "    (\"Pizza originated in Italy.\", False),                         # Completely irrelevant\n",
    "]\n",
    "\n",
    "print(\"Question: '{}'\\n\".format(question))\n",
    "print(\"Retrieved Chunks (with relevance):\")\n",
    "for i, (chunk, relevant) in enumerate(chunks_with_relevance,1):\n",
    "    status = \"‚úÖ Relevant\" if relevant else \"‚ùå Not relevant\"\n",
    "    print(f\"   {i}. {status}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f7f176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Manual Relevance Classification\n",
      "============================================================\n",
      "‚úÖ 'The Eiffel Tower is located in Paris, France....'-> RELEVANT\n",
      "‚úÖ 'Paris is the capital of France....'-> RELEVANT\n",
      "‚ùå 'The tower was built in 1889....'-> NOT RELEVANT\n",
      "‚ùå 'Pizza originated in Italy....'-> NOT RELEVANT\n"
     ]
    }
   ],
   "source": [
    "relevance_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Given the question and reference answer, determine if the following context chunk is RELEVANT.\n",
    "\n",
    "Question: {question}\n",
    "Reference Answer: {reference}\n",
    "Context Chunk: {chunk}\n",
    "\n",
    "Is this chunk relevant for answering the question? Answer only \"RELEVANT\" or \"NOT RELEVANT\".\n",
    "\"\"\")\n",
    "\n",
    "relevance_chain = relevance_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"üîç Manual Relevance Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "relevance_result = []\n",
    "for chunk,expected in chunks_with_relevance:\n",
    "    result = relevance_chain.invoke({\n",
    "        \"question\":question,\n",
    "        \"reference\":reference,\n",
    "        \"chunk\":chunk,\n",
    "    })\n",
    "    is_relevant = \"RELEVANT\" in result.upper() and \"NOT RELEVANT\" not in result.upper()\n",
    "    relevance_result.append(is_relevant)\n",
    "    status = \"‚úÖ\" if is_relevant else \"‚ùå\"\n",
    "    print(f\"{status} '{chunk[:50]}...'-> {result.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13eac520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä GOOD RANKING: Relevant chunks at TOP\n",
      "============================================================\n",
      "\n",
      "Ranking: [‚úÖ Relevant, ‚úÖ Relevant, ‚ùå Not Rel, ‚ùå Not Rel]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1 = 1/1 = ‚Üí Contributes\n",
      "   Position 2: Precision@2 = 2/2 = ‚Üí Contributes\n",
      "   Position 3: Precision@3 = 2/3 = ‚Üí Does NOT contribute\n",
      "   Position 4: Precision@4 = 2/4 = ‚Üí Does NOT contribute\n",
      "\n",
      "   Sum of contributing precisions: 2.00\n",
      "   Total relevant items: 2\n",
      "\n",
      "   üìä Context Precision (Good Ranking): 1.00\n"
     ]
    }
   ],
   "source": [
    "good_ranking = [True, True, False, False]\n",
    "\n",
    "print(\"üìä GOOD RANKING: Relevant chunks at TOP\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [‚úÖ Relevant, ‚úÖ Relevant, ‚ùå Not Rel, ‚ùå Not Rel]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_good = []\n",
    "relevant_count = 0\n",
    "\n",
    "for k, is_relevant in enumerate(good_ranking,1):\n",
    "    if is_relevant:\n",
    "        relevant_count +=1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contribute = \"‚Üí Contributes\" if is_relevant else \"‚Üí Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k} = {relevant_count}/{k} = {contribute}\")\n",
    "    if is_relevant:\n",
    "        precisions_good.append(precision_at_k)\n",
    "\n",
    "total_relevant = sum(good_ranking)\n",
    "context_precision_good = sum(precisions_good)/total_relevant if total_relevant >0 else 0\n",
    "\n",
    "print(f\"\\n   Sum of contributing precisions: {sum(precisions_good):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   üìä Context Precision (Good Ranking): {context_precision_good:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38bbc8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä BAD RANKING: Relevant chunks at BOTTOM\n",
      "============================================================\n",
      "\n",
      "Ranking: [‚ùå Not Rel, ‚ùå Not Rel, ‚úÖ Relevant, ‚úÖ Relevant]\n",
      "\n",
      "Precision@K calculation:\n",
      "   Position 1: Precision@1=0/1 = 0.0\n",
      "   Position 2: Precision@2=0/2 = 0.0\n",
      "   Position 3: Precision@3=1/3 = 0.3333333333333333\n",
      "   Position 4: Precision@4=2/4 = 0.5\n",
      "\n",
      "   Sum of contributing precision: 0.83\n",
      "   Total relevant items: 2\n",
      "\n",
      "   üìä Context Precision (Bad Ranking): 0.42\n"
     ]
    }
   ],
   "source": [
    "bad_ranking = [False, False, True, True]\n",
    "\n",
    "print(\"üìä BAD RANKING: Relevant chunks at BOTTOM\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRanking: [‚ùå Not Rel, ‚ùå Not Rel, ‚úÖ Relevant, ‚úÖ Relevant]\")\n",
    "print(\"\\nPrecision@K calculation:\")\n",
    "\n",
    "precisions_bad = []\n",
    "relevant_count = 0\n",
    "\n",
    "for k, is_relevant in enumerate(bad_ranking,1):\n",
    "    if is_relevant:\n",
    "        relevant_count+=1\n",
    "    precision_at_k = relevant_count / k\n",
    "    contributes = \"‚Üí Contributes\" if is_relevant else \"‚Üí Does NOT contribute\"\n",
    "    print(f\"   Position {k}: Precision@{k}={relevant_count}/{k} = {precision_at_k}\")\n",
    "\n",
    "    if is_relevant:\n",
    "        precisions_bad.append(precision_at_k)\n",
    "total_relevant = sum(bad_ranking)\n",
    "context_precision_bad = sum(precisions_bad)/total_relevant if total_relevant > 0 else 0 \n",
    "\n",
    "print(f\"\\n   Sum of contributing precision: {sum(precisions_bad):.2f}\")\n",
    "print(f\"   Total relevant items: {total_relevant}\")\n",
    "print(f\"\\n   üìä Context Precision (Bad Ranking): {context_precision_bad:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f147d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Context Precision Results\n",
      "==================================================\n",
      "\n",
      " Good Ranking (relevant at top): 1.00\n",
      "   Bad Ranking (relevant at botton): 0.42\n",
      "\n",
      "   Difference: 0.58\n"
     ]
    }
   ],
   "source": [
    "good_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is located in Paris, France.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Pizza originated in Italy.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "bad_sample = SingleTurnSample(\n",
    "    user_input=question,\n",
    "    reference=reference,\n",
    "    retrieved_contexts=[\n",
    "        \"Pizza originated in Italy.\",\n",
    "        \"The tower was built in 1889.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"The Eiffel Tower is located in Paris, France.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "precision_metrics = LLMContextPrecisionWithReference(llm=ragas_llm)\n",
    "\n",
    "good_score = run_async(precision_metrics.single_turn_ascore(good_sample))\n",
    "bad_score = run_async(precision_metrics.single_turn_ascore(bad_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Context Precision Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Good Ranking (relevant at top): {good_score:.2f}\")\n",
    "print(f\"   Bad Ranking (relevant at botton): {bad_score:.2f}\")\n",
    "print(f\"\\n   Difference: {good_score-bad_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30c0aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Reference Answer (Ground Truth):\n",
      "   'The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.'\n",
      "\n",
      "üìö Retrieved Context:\n",
      "   1. 'The Eiffel Tower is a landmark located in Paris, France.'\n",
      "   2. 'The tower was completed in 1889 for the World's Fair.'\n"
     ]
    }
   ],
   "source": [
    "recall_question = \"Tell me about the Eiffel Tower.\"\n",
    "recall_reference = \"The Eiffel Tower is located in Paris. It was built in 1889. It is 330 meters tall.\"\n",
    "\n",
    "# Retrieved context (missing the height information)\n",
    "recall_context = [\n",
    "    \"The Eiffel Tower is a landmark located in Paris, France.\",\n",
    "    \"The tower was completed in 1889 for the World's Fair.\"\n",
    "]\n",
    "print(\"üìù Reference Answer (Ground Truth):\")\n",
    "print(f\"   '{recall_reference}'\")\n",
    "print(\"\\nüìö Retrieved Context:\")\n",
    "for i, ctx in enumerate(recall_context, 1):\n",
    "    print(f\"   {i}. '{ctx}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a6d84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 1: Reference Claims\n",
      "==================================================\n",
      "   1. The Eiffel Tower is located in Paris\n",
      "   2. It was built in 1889\n",
      "   3. It is 330 meters tall\n"
     ]
    }
   ],
   "source": [
    "reference_claims = [\n",
    "    \"The Eiffel Tower is located in Paris\",\n",
    "    \"It was built in 1889\",\n",
    "    \"It is 330 meters tall\"\n",
    "]\n",
    "\n",
    "print(\"üîç STEP 1: Reference Claims\")\n",
    "print(\"=\" * 50)\n",
    "for i, claim in enumerate(reference_claims, 1):\n",
    "    print(f\"   {i}. {claim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d7366520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 2: Claim Attribution Check\n",
      "============================================================\n",
      "[True]\n",
      "   ‚úÖ Found:  'The Eiffel Tower is located in Paris'\n",
      "[True, True]\n",
      "   ‚úÖ Found:  'It was built in 1889'\n",
      "[True, True, False]\n",
      "   ‚ùå MISSING:  'It is 330 meters tall'\n",
      "      ‚ö†Ô∏è This information was NOT retrieved!\n"
     ]
    }
   ],
   "source": [
    "attribution_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Can the following claim be attributed to (found in) the given context?\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Claim: {claim}\n",
    "\n",
    "Answer \"YES\" if the claim is supported by the context, \"NO\" if it cannot be found.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "attribution_chain = attribution_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"üîç STEP 2: Claim Attribution Check\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "combined_context = \"\\n\".join(recall_context)\n",
    "attribution_result=[]\n",
    "\n",
    "for claim in reference_claims:\n",
    "    result = attribution_chain.invoke({\n",
    "        \"context\":combined_context,\n",
    "        \"claim\":claim\n",
    "    })\n",
    "    found = \"YES\" in result.upper()\n",
    "    attribution_result.append(found)\n",
    "    print(attribution_result)\n",
    "    status = \"‚úÖ Found\" if found else \"‚ùå MISSING\"\n",
    "    print(f\"   {status}:  '{claim}'\")\n",
    "    if not found:\n",
    "        print(f\"      ‚ö†Ô∏è This information was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "56dd04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ STEP 3: Calculate Context Recall\n",
      "==================================================\n",
      "\n",
      "   Claims found in context: 2\n",
      "   Total claims in reference: 3\n",
      "\n",
      "   Formula: 2 / 3 = 0.67\n",
      "\n",
      "   üìä Context Recall: 0.67\n",
      "\n",
      "   ‚ö†Ô∏è Interpretation: 33% of required info was NOT retrieved!\n"
     ]
    }
   ],
   "source": [
    "claims_found = sum(attribution_result)\n",
    "total_claims = len(reference_claims)\n",
    "manual_recall = claims_found / total_claims\n",
    "\n",
    "print(\"üî¢ STEP 3: Calculate Context Recall\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Claims found in context: {claims_found}\")\n",
    "print(f\"   Total claims in reference: {total_claims}\")\n",
    "print(f\"\\n   Formula: {claims_found} / {total_claims} = {manual_recall:.2f}\")\n",
    "print(f\"\\n   üìä Context Recall: {manual_recall:.2f}\")\n",
    "print(f\"\\n   ‚ö†Ô∏è Interpretation: {100 - manual_recall*100:.0f}% of required info was NOT retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7106783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Context Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual calculation: 0.67\n",
      "   RAGAS metric score: 0.67\n"
     ]
    }
   ],
   "source": [
    "recall_sample = SingleTurnSample(\n",
    "    user_input=recall_question,\n",
    "    response=\"The Eiffel Tower is in Paris and was built in 1889.\",\n",
    "    reference=recall_reference,\n",
    "    retrieved_contexts=recall_context\n",
    ")\n",
    "\n",
    "recall_metric = LLMContextRecall(llm=ragas_llm)\n",
    "ragas_recall = run_async(recall_metric.single_turn_ascore(recall_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Context Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual calculation: {manual_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f85f762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Reference Answer:\n",
      "   'Albert Einstein developed the theory of relativity at Princeton University in 1905.'\n",
      "\n",
      "üìö Retrieved Context:\n",
      "   'Albert Einstein was a famous physicist who worked at Princeton.'\n"
     ]
    }
   ],
   "source": [
    "entity_reference = \"Albert Einstein developed the theory of relativity at Princeton University in 1905.\"\n",
    "entity_context = [\n",
    "    \"Albert Einstein was a famous physicist who worked at Princeton.\"\n",
    "]\n",
    "\n",
    "print(\"üìù Reference Answer:\")\n",
    "print(f\"   '{entity_reference}'\")\n",
    "print(\"\\nüìö Retrieved Context:\")\n",
    "print(f\"   '{entity_context[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4fc2f50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Entity Extraction\n",
      "============================================================\n",
      "   Referance Entity:\n",
      "Albert Einstein - PERSON  \n",
      "Princeton University - ORGANIZATION  \n",
      "1905 - DATE  \n",
      "\n",
      "   Context Entities: \n",
      "Albert Einstein - PERSON  \n",
      "Princeton - LOCATION\n"
     ]
    }
   ],
   "source": [
    "entity_extraction_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Extract all named entities from the following text. \n",
    "Include: PERSON, ORGANIZATION, LOCATION, DATE, and other proper nouns.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "List each entity on a new line with its type:\n",
    "\"\"\"\n",
    "\n",
    ")\n",
    "entity_chain = entity_extraction_prompt | llm | StrOutputParser()\n",
    "print(\"üîç Entity Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"   Referance Entity:\")\n",
    "ref_entities = entity_chain.invoke({\n",
    "    \"text\":entity_reference\n",
    "})\n",
    "print(ref_entities)\n",
    "\n",
    "print(\"\\n   Context Entities: \")\n",
    "ctx_entities = entity_chain.invoke(\n",
    "    {\"text\":entity_context[0]}\n",
    ")\n",
    "print(ctx_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27a606bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Entity Comparison\n",
      "============================================================\n",
      "\n",
      "| Entity in Reference  |     Type     |Found in Context? |\n",
      "|----------------------|--------------|------------------|\n",
      "| Albert Einstein      | PERSON       |‚úÖ Yes           |\n",
      "| Princeton University | ORGANIZATION |‚úÖ Yes           |\n",
      "| 1905                 | DATE         |‚ùå MISSING       |\n",
      "\n",
      "   Entity Recall: 2/3 = 0.67\n",
      "‚ö†Ô∏è Missing: '1905' - Critical date not retrieved!\n"
     ]
    }
   ],
   "source": [
    "reference_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton University\": \"ORGANIZATION\",\n",
    "    \"1905\": \"DATE\"\n",
    "}\n",
    "\n",
    "context_entities = {\n",
    "    \"Albert Einstein\": \"PERSON\",\n",
    "    \"Princeton\": \"ORGANIZATION\"  # Partial match\n",
    "}\n",
    "\n",
    "print(\"üìä Entity Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n| Entity in Reference  |     Type     |Found in Context? |\")\n",
    "print(\"|\" + \"-\" * 22 + \"|\" + \"-\" * 14 + \"|\" + \"-\" * 18 + \"|\")\n",
    "\n",
    "found_count = 0\n",
    "for entity, entity_type in reference_entities.items():\n",
    "    found = any(entity.lower() in ctx.lower() or ctx.lower() in entity.lower() for ctx in context_entities.keys())\n",
    "    if found:\n",
    "        found_count+=1\n",
    "    status = \"‚úÖ Yes\" if found else \"‚ùå MISSING\"\n",
    "    print(f\"| {entity:20} | {entity_type:12} |{status:16}|\")\n",
    "\n",
    "entity_recall = found_count / len(reference_entities)\n",
    "print(f\"\\n   Entity Recall: {found_count}/{len(reference_entities)} = {entity_recall:.2f}\")\n",
    "print(f\"‚ö†Ô∏è Missing: '1905' - Critical date not retrieved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f5b7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Context Entity Recall Result\n",
      "==================================================\n",
      "\n",
      "   Manual estimate: 0.67\n",
      "   RAGAS metric score: 0.25\n"
     ]
    }
   ],
   "source": [
    "entity_sample = SingleTurnSample(\n",
    "    reference=entity_reference,\n",
    "    retrieved_contexts=entity_context\n",
    ")\n",
    "entity_metric = ContextEntityRecall(llm=ragas_llm)\n",
    "ragas_entity_recall = run_async(entity_metric.single_turn_ascore(entity_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Context Entity Recall Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Manual estimate: {entity_recall:.2f}\")\n",
    "print(f\"   RAGAS metric score: {ragas_entity_recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "828564b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Question: 'What is LIC known for?'\n",
      "\n",
      "üìù Response to evaluate:\n",
      "   'LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.'\n",
      "\n",
      "üìù Reference (Ground Truth):\n",
      "   'LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.'\n",
      "\n",
      "üìö Retrieved Contexts:\n",
      "   1. 'LIC was established in 1956 following nationalization....' ‚úÖ\n",
      "   2. 'LIC is the largest insurance company with huge investments....' ‚úÖ\n",
      "   3. 'LIC manages substantial funds for financial stability....' ‚úÖ\n",
      "   4. 'The Indian economy is one of the fastest-growing economies.....' ‚Üê NOISE!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_question = \"What is LIC known for?\"\n",
    "noise_response = \"LIC is the largest insurance company in India, known for its vast portfolio. LIC contributes to financial stability.\"\n",
    "noise_reference = \"LIC is the largest insurance company in India, established in 1956. It is known for managing a large portfolio of investments.\"\n",
    "\n",
    "noise_contexts = [\n",
    "    \"LIC was established in 1956 following nationalization.\",           # ‚úÖ Relevant\n",
    "    \"LIC is the largest insurance company with huge investments.\",      # ‚úÖ Relevant\n",
    "    \"LIC manages substantial funds for financial stability.\",           # ‚úÖ Relevant\n",
    "    \"The Indian economy is one of the fastest-growing economies...\"     # ‚ùå NOISE!\n",
    "]\n",
    "\n",
    "print(\"üìù Question: '{}'\\n\".format(noise_question))\n",
    "print(\"üìù Response to evaluate:\")\n",
    "print(f\"   '{noise_response}'\")\n",
    "print(\"\\nüìù Reference (Ground Truth):\")\n",
    "print(f\"   '{noise_reference}'\")\n",
    "print(\"\\nüìö Retrieved Contexts:\")\n",
    "for i, ctx in enumerate(noise_contexts, 1):\n",
    "    noise_tag = \" ‚Üê NOISE!\" if i == 4 else \" ‚úÖ\"\n",
    "    print(f\"   {i}. '{ctx[:60]}...'{noise_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5bff7e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Claim Analysis\n",
      "======================================================================\n",
      "\n",
      "| Claim | Correct? | Reason |\n",
      "|---------------------------------------------|----------|----------------------------------------|\n",
      "| LIC is the largest insurance company in Ind | ‚úÖ Yes    | Matches reference                      |\n",
      "| LIC is known for its vast portfolio         | ‚úÖ Yes    | Matches reference (portfolio)          |\n",
      "| LIC contributes to financial stability      | ‚ùå No     | NOT in reference - possible hallucinat |\n"
     ]
    }
   ],
   "source": [
    "# Analyze claims in response\n",
    "\n",
    "response_claims = [\n",
    "    (\"LIC is the largest insurance company in India\", True, \"Matches reference\"),\n",
    "    (\"LIC is known for its vast portfolio\", True, \"Matches reference (portfolio)\"),\n",
    "    (\"LIC contributes to financial stability\", False, \"NOT in reference - possible hallucination from noise!\")\n",
    "]\n",
    "\n",
    "print(\"üîç Claim Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n| Claim | Correct? | Reason |\")\n",
    "print(\"|\" + \"-\" * 45 + \"|\" + \"-\" * 10 + \"|\" + \"-\" * 40 + \"|\")\n",
    "\n",
    "incorrect_count = 0\n",
    "for claim, is_correct, reason in response_claims:\n",
    "    status = \"‚úÖ Yes\" if is_correct else \"‚ùå No\"\n",
    "    if not is_correct:\n",
    "        incorrect_count += 1\n",
    "    print(f\"| {claim[:43]:43} | {status:8} | {reason[:38]:38} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acfa05ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Noise Sensitivity Calculation\n",
      "==================================================\n",
      "\n",
      "   Incorrect claims: 1\n",
      "   Total claims: 3\n",
      "\n",
      "   Formula: 1 / 3 = 0.33\n",
      "\n",
      "   üìä Noise Sensitivity: 0.33\n",
      "   ‚ö†Ô∏è Warning! Model is sometimes confused by noise.\n"
     ]
    }
   ],
   "source": [
    "total_claims = len(response_claims)\n",
    "noise_sensitivity = incorrect_count / total_claims\n",
    "\n",
    "print(\"üî¢ Noise Sensitivity Calculation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Incorrect claims: {incorrect_count}\")\n",
    "print(f\"   Total claims: {total_claims}\")\n",
    "print(f\"\\n   Formula: {incorrect_count} / {total_claims} = {noise_sensitivity:.2f}\")\n",
    "print(f\"\\n   üìä Noise Sensitivity: {noise_sensitivity:.2f}\")\n",
    "\n",
    "if noise_sensitivity < 0.3:\n",
    "    print(\"   ‚úÖ Good! Model is mostly resistant to noise.\")\n",
    "elif noise_sensitivity < 0.6:\n",
    "    print(\"   ‚ö†Ô∏è Warning! Model is sometimes confused by noise.\")\n",
    "else:\n",
    "    print(\"   üö® Bad! Model is highly susceptible to noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10db640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ RAGAS Noise Sensitivity Result\n",
      "==================================================\n",
      "\n",
      "   Mode: relevant\n",
      "   Score: 0.33\n",
      "\n",
      "   Remember: Lower is better for this metric!\n"
     ]
    }
   ],
   "source": [
    "noise_sample = SingleTurnSample(\n",
    "    user_input=noise_question,\n",
    "    response=noise_response,\n",
    "    reference=noise_reference,\n",
    "    retrieved_contexts=noise_contexts\n",
    ")\n",
    "noise_metric_relevant = NoiseSensitivity(llm=ragas_llm, mode=\"relevant\")\n",
    "ragas_noise = run_async(noise_metric_relevant.single_turn_ascore(noise_sample))\n",
    "\n",
    "print(\"üî¨ RAGAS Noise Sensitivity Result\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n   Mode: relevant\")\n",
    "print(f\"   Score: {ragas_noise:.2f}\")\n",
    "print(f\"\\n   Remember: Lower is better for this metric!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c79f9c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Complete Sample for Evaluation\n",
      "============================================================\n",
      "\n",
      "Question: What is the Eiffel Tower and where is it located?\n",
      "\n",
      "Response: The Eiffel Tower is a famous iron lattice tower located in Paris, France. It was built in 1889.\n",
      "\n",
      "Referance: The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was constructed from 1887 to 1889.\n",
      "\n",
      "Contexts: 4\n"
     ]
    }
   ],
   "source": [
    "complete_sample = SingleTurnSample(\n",
    "    user_input=\"What is the Eiffel Tower and where is it located?\",\n",
    "    response=\"The Eiffel Tower is a famous iron lattice tower located in Paris, France. It was built in 1889.\",\n",
    "    reference=\"The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was constructed from 1887 to 1889.\",\n",
    "    retrieved_contexts=[\n",
    "        \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
    "        \"The tower was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair.\",\n",
    "        \"The Eiffel Tower is named after Gustave Eiffel, whose company designed and built the tower.\",\n",
    "        \"Paris is known for its cafe culture and fashion industry.\"  # Some noise\n",
    "    ]\n",
    ")\n",
    "print(\"üìä Complete Sample for Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuestion: {complete_sample.user_input}\")\n",
    "print(f\"\\nResponse: {complete_sample.response}\")\n",
    "print(f\"\\nReferance: {complete_sample.reference}\")\n",
    "print(f\"\\nContexts: {len(complete_sample.retrieved_contexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3f25c4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running All 6 RAGAS Metrics\n",
      "============================================================\n",
      "\n",
      "   Faithfulness: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "   Answer Relevancy: 0.970 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "   Context Precision: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "   Context Recall: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "   Context Entity Recall: 1.000 (higher is better)\n",
      "   Assessment: Good\n",
      "\n",
      "   Noise Sensitivity: 0.000 (lower is better)\n",
      "   Assessment: Good\n"
     ]
    }
   ],
   "source": [
    "print(\"üî¨ Running All 6 RAGAS Metrics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_metric = {\n",
    "    \"Faithfulness\":Faithfulness(llm=ragas_llm),\n",
    "    \"Answer Relevancy\": ResponseRelevancy(llm=ragas_llm,embeddings=ragas_embeddings),\n",
    "    \"Context Precision\": LLMContextPrecisionWithReference(llm=ragas_llm),\n",
    "    \"Context Recall\": LLMContextRecall(llm=ragas_llm),\n",
    "    \"Context Entity Recall\":ContextEntityRecall(llm=ragas_llm),\n",
    "    \"Noise Sensitivity\":NoiseSensitivity(llm=ragas_llm)\n",
    "}\n",
    "result = {}\n",
    "for name, metric in all_metric.items():\n",
    "    try:\n",
    "        score = run_async(metric.single_turn_ascore(complete_sample))\n",
    "        result[name] = score\n",
    "\n",
    "        if name == \"Noise Sensitivity\":\n",
    "            quality = \"Good\" if score < 0.3 else \"Concerning\" if score < 0.6 else \"Poor\"\n",
    "            direction = \"(lower is better)\"\n",
    "        else:\n",
    "            quality = \"Good\" if score > 0.7 else \"Concerning\" if score > 0.5 else \"Poor\"\n",
    "            direction = \"(higher is better)\"\n",
    "        print(f\"\\n   {name}: {score:.3f} {direction}\")\n",
    "        print(f\"   Assessment: {quality}\")\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  {name}: Error - {str(e)[:50]}\")\n",
    "        result[name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ecbd5503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Metric    Score Ideal Status\n",
      "         Faithfulness 1.000000   1.0      ‚úÖ\n",
      "     Answer Relevancy 0.969854   1.0      ‚úÖ\n",
      "    Context Precision 1.000000   1.0      ‚úÖ\n",
      "       Context Recall 1.000000   1.0      ‚úÖ\n",
      "Context Entity Recall 1.000000   1.0      ‚úÖ\n",
      "    Noise Sensitivity 0.000000   0.0      ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "summary_data = []\n",
    "for name, score in result.items():\n",
    "    if score is not None:\n",
    "        if name == \"Noise Sensitivity\":\n",
    "            ideal = \"0.0\"\n",
    "            status = \"‚úÖ\" if score<0.3 else \"‚ö†Ô∏è\" if score < 0.6 else \"‚ùå\"\n",
    "        else:\n",
    "            ideal = \"1.0\"\n",
    "            status = \"‚úÖ\" if score > 0.7 else \"‚ö†Ô∏è\" if score >0.5 else \"‚ùå\"\n",
    "        summary_data.append({\n",
    "            \"Metric\":name,\n",
    "            \"Score\": score,\n",
    "            \"Ideal\": ideal,\n",
    "            \"Status\": status\n",
    "        }) \n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "138af248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Created 3 test samples for batch evaluation\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of test samples\n",
    "\n",
    "test_samples = [\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What is RAG?\",\n",
    "        response=\"RAG stands for Retrieval Augmented Generation. It combines retrieval systems with LLMs to provide accurate, grounded responses.\",\n",
    "        reference=\"RAG (Retrieval Augmented Generation) is a technique that enhances LLM responses by retrieving relevant documents and using them as context.\",\n",
    "        retrieved_contexts=[\n",
    "            \"RAG combines retrieval with generation for accurate responses.\",\n",
    "            \"Retrieval Augmented Generation uses external knowledge bases.\"\n",
    "        ]\n",
    "    ),\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What are embeddings?\",\n",
    "        response=\"Embeddings are numerical vector representations of text that capture semantic meaning.\",\n",
    "        reference=\"Embeddings are dense vector representations that encode semantic information about text into numerical format.\",\n",
    "        retrieved_contexts=[\n",
    "            \"Embeddings convert text to dense vectors.\",\n",
    "            \"Vector representations capture semantic similarity.\"\n",
    "        ]\n",
    "    ),\n",
    "    SingleTurnSample(\n",
    "        user_input=\"What is chunking?\",\n",
    "        response=\"Chunking is the process of breaking documents into smaller pieces for processing.\",\n",
    "        reference=\"Chunking divides large documents into smaller segments that can be individually embedded and retrieved.\",\n",
    "        retrieved_contexts=[\n",
    "            \"Document chunking breaks text into manageable pieces.\",\n",
    "            \"Chunk size affects retrieval quality.\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"üìä Created {len(test_samples)} test samples for batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e489f2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Running Batch Evaluation...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067a936920549d88e7093e120fe1225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Batch evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset(samples=test_samples)\n",
    "\n",
    "batch_metrics = [\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "    LLMContextRecall(llm=ragas_llm)\n",
    "]\n",
    "\n",
    "print(\"üî¨ Running Batch Evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_result = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=batch_metrics\n",
    ")\n",
    "print(\"\\n‚úÖ Batch evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c231320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Batch Evaluation Results\n",
      "============================================================\n",
      "             user_input                                                                                                               retrieved_contexts                                                                                                                         response                                                                                                                                    reference  faithfulness  answer_relevancy  context_recall\n",
      "0          What is RAG?  [RAG combines retrieval with generation for accurate responses., Retrieval Augmented Generation uses external knowledge bases.]  RAG stands for Retrieval Augmented Generation. It combines retrieval systems with LLMs to provide accurate, grounded responses.  RAG (Retrieval Augmented Generation) is a technique that enhances LLM responses by retrieving relevant documents and using them as context.      0.666667          0.940876             0.0\n",
      "1  What are embeddings?                                 [Embeddings convert text to dense vectors., Vector representations capture semantic similarity.]                                           Embeddings are numerical vector representations of text that capture semantic meaning.                               Embeddings are dense vector representations that encode semantic information about text into numerical format.      1.000000          1.000000             1.0\n",
      "2     What is chunking?                                   [Document chunking breaks text into manageable pieces., Chunk size affects retrieval quality.]                                                Chunking is the process of breaking documents into smaller pieces for processing.                                      Chunking divides large documents into smaller segments that can be individually embedded and retrieved.      0.500000          1.000000             0.0\n",
      "\n",
      "üìà Average Scores:\n",
      "   faithfulness: 0.722\n",
      "   answer_relevancy: 0.980\n",
      "   context_recall: 0.333\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Batch Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result_df = batch_result.to_pandas()\n",
    "print(result_df.to_string())\n",
    "\n",
    "print(\"\\nüìà Average Scores:\")\n",
    "for col in result_df.columns:\n",
    "    if col not in ['user_input', 'response', 'reference', 'retrieved_contexts']:\n",
    "        avg = result_df[col].mean()\n",
    "        print(f\"   {col}: {avg:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
