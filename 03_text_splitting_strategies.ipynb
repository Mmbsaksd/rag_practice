{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657f3805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea372c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 12 chunks\n",
      "\n",
      "[Document(metadata={'source': 'sample_data/notes.txt'}, page_content='LANGCHAIN STUDY NOTES - RAG IMPLEMENTATION\\n==========================================\\n\\nDate: January 15, 2025\\nTopic: Retrieval-Augmented Generation with LangChain 1.0+\\n\\n\\nCORE CONCEPTS\\n-------------\\n\\n1. Document Object Structure\\n   - page_content: The actual text content\\n   - metadata: Dictionary with additional information (source, page, date, etc.)\\n   - id: Unique identifier (optional)\\n\\n2. LCEL (LangChain Expression Language)\\n   - Uses pipe operator | to chain components\\n   - More readable than nested function calls\\n   - Better error handling and debugging\\n   - Example: retriever | format_docs | prompt | llm | parser\\n\\n3. Vector Similarity Search\\n   - Converts text to high-dimensional vectors (embeddings)\\n   - Uses distance metrics: cosine similarity, Euclidean distance, dot product\\n   - Finds semantically similar documents, not just keyword matches\\n   - Typical embedding dimensions: 384 (small), 768 (medium), 1536 (large)\\n\\n\\nTEXT SPLITTING STRATEGIES\\n--------------------------'), Document(metadata={'source': 'sample_data/notes.txt'}, page_content='TEXT SPLITTING STRATEGIES\\n--------------------------\\n\\nRecursiveCharacterTextSplitter (RECOMMENDED)\\n- Tries to split on semantic boundaries\\n- Order: double newline √¢‚Ä†‚Äô newline √¢‚Ä†‚Äô period √¢‚Ä†‚Äô space √¢‚Ä†‚Äô character\\n- Best for general text and documentation\\n- Configuration: chunk_size=1000, chunk_overlap=200\\n\\nCharacterTextSplitter\\n- Simple splitting on a single separator\\n- Use for basic cases or when speed is critical\\n- Less sophisticated than recursive splitter\\n\\nHTMLHeaderTextSplitter\\n- Splits HTML based on header tags (h1, h2, h3)\\n- Preserves document structure in metadata\\n- Ideal for web content and documentation\\n\\nTokenTextSplitter\\n- Splits based on token count, not characters\\n- More accurate for LLM context window limits\\n- Uses tiktoken for OpenAI models\\n\\n\\nCHUNK SIZE GUIDELINES\\n----------------------'), Document(metadata={'source': 'sample_data/notes.txt'}, page_content='TokenTextSplitter\\n- Splits based on token count, not characters\\n- More accurate for LLM context window limits\\n- Uses tiktoken for OpenAI models\\n\\n\\nCHUNK SIZE GUIDELINES\\n----------------------\\n\\nContent Type          | Chunk Size | Overlap | Notes\\n----------------------|------------|---------|---------------------------\\nGeneral Text          | 1000 chars | 200     | Default recommendation\\nTechnical Docs        | 500-800    | 100-150 | Precision over context\\nSource Code           | 200-400    | 50-100  | Function/class level\\nLong Articles         | 1500-2000  | 300     | More context needed\\nConversational Data   | 100-200    | 20      | Short exchanges\\n\\n\\nEMBEDDING MODELS COMPARISON\\n----------------------------\\n\\nOpenAI text-embedding-3-small\\n- Dimensions: 1536\\n- Cost: $0.00002 per 1K tokens\\n- MTEB Score: 62.3%\\n- Best for: Production applications with budget')]\n",
      "======================================================================\n",
      "Chunk 1: 991 chars\n",
      "======================================================================\n",
      "LANGCHAIN STUDY NOTES - RAG IMPLEMENTATION\n",
      "==========================================\n",
      "\n",
      "Date: January 15, 2025\n",
      "Topic: Retrieval-Augmented Generation with LangChain 1.0+\n",
      "\n",
      "\n",
      "CORE CONCEPTS\n",
      "-------------\n",
      "\n",
      "1. Document Object Structure\n",
      "   - page_content: The actual text content\n",
      "   - metadata: Dictionary wit\n",
      "\n",
      "======================================================================\n",
      "Chunk 2: 809 chars\n",
      "======================================================================\n",
      "TEXT SPLITTING STRATEGIES\n",
      "--------------------------\n",
      "\n",
      "RecursiveCharacterTextSplitter (RECOMMENDED)\n",
      "- Tries to split on semantic boundaries\n",
      "- Order: double newline √¢‚Ä†‚Äô newline √¢‚Ä†‚Äô period √¢‚Ä†‚Äô space √¢‚Ä†‚Äô character\n",
      "- Best for general text and documentation\n",
      "- Configuration: chunk_size=1000, chunk_overlap=\n",
      "\n",
      "======================================================================\n",
      "Chunk 3: 864 chars\n",
      "======================================================================\n",
      "TokenTextSplitter\n",
      "- Splits based on token count, not characters\n",
      "- More accurate for LLM context window limits\n",
      "- Uses tiktoken for OpenAI models\n",
      "\n",
      "\n",
      "CHUNK SIZE GUIDELINES\n",
      "----------------------\n",
      "\n",
      "Content Type          | Chunk Size | Overlap | Notes\n",
      "----------------------|------------|---------|---------\n",
      "\n",
      "======================================================================\n",
      "Chunk 4: 911 chars\n",
      "======================================================================\n",
      "OpenAI text-embedding-3-small\n",
      "- Dimensions: 1536\n",
      "- Cost: $0.00002 per 1K tokens\n",
      "- MTEB Score: 62.3%\n",
      "- Best for: Production applications with budget\n",
      "\n",
      "OpenAI text-embedding-3-large\n",
      "- Dimensions: 3072\n",
      "- Cost: $0.00013 per 1K tokens\n",
      "- MTEB Score: 64.6%\n",
      "- Best for: Highest quality requirements\n",
      "\n",
      "HuggingFa\n",
      "\n",
      "======================================================================\n",
      "Chunk 5: 962 chars\n",
      "======================================================================\n",
      "InMemoryVectorStore\n",
      "Pros: Simple setup, no dependencies, fast for small datasets\n",
      "Cons: No persistence, limited to memory size\n",
      "Use for: Testing, development, small demos\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "Pros: Very fast, scales to billions, runs locally, persistent\n",
      "Cons: Limited metadata filteri\n",
      "\n",
      "======================================================================\n",
      "Chunk 6: 965 chars\n",
      "======================================================================\n",
      "Pinecone\n",
      "Pros: Fully managed, highly scalable, great performance\n",
      "Cons: Costs money, vendor lock-in, requires internet\n",
      "Use for: Production apps where managed service is preferred\n",
      "\n",
      "\n",
      "RETRIEVAL STRATEGIES\n",
      "---------------------\n",
      "\n",
      "Similarity Search\n",
      "- Default method: returns top-k most similar documents\n",
      "- F\n",
      "\n",
      "======================================================================\n",
      "Chunk 7: 764 chars\n",
      "======================================================================\n",
      "COMMON MISTAKES TO AVOID\n",
      "-------------------------\n",
      "\n",
      "1. Hardcoding API keys √¢‚Ä†‚Äô Use environment variables\n",
      "2. Not persisting vector stores √¢‚Ä†‚Äô Save and reuse to avoid re-embedding\n",
      "3. Ignoring metadata √¢‚Ä†‚Äô Maintain source information for citation\n",
      "4. Random chunk sizes √¢‚Ä†‚Äô Choose based on content type a\n",
      "\n",
      "======================================================================\n",
      "Chunk 8: 904 chars\n",
      "======================================================================\n",
      "PRODUCTION CHECKLIST\n",
      "---------------------\n",
      "\n",
      "Before Deployment:\n",
      "√¢‚Äì¬° Environment variables configured (.env file)\n",
      "√¢‚Äì¬° API keys secured (not in code)\n",
      "√¢‚Äì¬° Vector store persistent (saved to disk)\n",
      "√¢‚Äì¬° Error handling implemented\n",
      "√¢‚Äì¬° Costs estimated and monitored\n",
      "√¢‚Äì¬° Retrieval quality tested\n",
      "√¢‚Äì¬° Rate limiti\n",
      "\n",
      "======================================================================\n",
      "Chunk 9: 920 chars\n",
      "======================================================================\n",
      "chain = (\n",
      "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
      "    | prompt\n",
      "    | llm\n",
      "    | StrOutputParser()\n",
      ")\n",
      "\n",
      "answer = chain.invoke(\"What is RAG?\")\n",
      "```\n",
      "\n",
      "Loading Multiple File Types:\n",
      "```python\n",
      "from pathlib import Path\n",
      "from langchain_community.document_loaders import (\n",
      "    Py\n",
      "\n",
      "======================================================================\n",
      "Chunk 10: 644 chars\n",
      "======================================================================\n",
      "for doc, score in results:\n",
      "    print(f\"Score: {score:.4f} | {doc.page_content[:100]}...\")\n",
      "```\n",
      "\n",
      "\n",
      "RESOURCES\n",
      "---------\n",
      "\n",
      "Official Documentation:\n",
      "- LangChain Docs: https://python.langchain.com/docs/\n",
      "- OpenAI API: https://platform.openai.com/docs\n",
      "- FAISS: https://github.com/facebookresearch/faiss\n",
      "\n",
      "Tutoria\n",
      "\n",
      "======================================================================\n",
      "Chunk 11: 609 chars\n",
      "======================================================================\n",
      "Community:\n",
      "- LangChain Discord\n",
      "- r/LangChain on Reddit\n",
      "- Stack Overflow (tag: langchain)\n",
      "\n",
      "\n",
      "NEXT STEPS\n",
      "----------\n",
      "\n",
      "1. Practice with different document types (PDF, CSV, JSON, HTML)\n",
      "2. Experiment with chunk sizes for your specific use case\n",
      "3. Compare embedding models (OpenAI vs HuggingFace)\n",
      "4. Try diff\n",
      "\n",
      "======================================================================\n",
      "Chunk 12: 552 chars\n",
      "======================================================================\n",
      "KEY TAKEAWAYS\n",
      "-------------\n",
      "\n",
      "√¢≈ì‚Äú RAG = Retrieval + Context + Generation\n",
      "√¢≈ì‚Äú Good chunking strategy is critical for quality\n",
      "√¢≈ì‚Äú LCEL makes chain building more readable\n",
      "√¢≈ì‚Äú Always persist vector stores to save costs\n",
      "√¢≈ì‚Äú Test retrieval quality before optimizing generation\n",
      "√¢≈ì‚Äú Monitor costs throughout d\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "txt_path = 'sample_data/notes.txt'\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    loader = TextLoader(txt_path)\n",
    "    doc = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\".\",\" \",\"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(doc)\n",
    "    print(f\"Split into {len(chunks)} chunks\\n\")\n",
    "\n",
    "    print(chunks[:3])\n",
    "    for i, chunk in enumerate(chunks,1):\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Chunk {i}: {len(chunk.page_content)} chars\")\n",
    "        print(\"=\"*70)\n",
    "        print(chunk.page_content[:300] if len(chunk.page_content)>300 else chunk.page_content)\n",
    "        print()\n",
    "else:\n",
    "    print(f\"File not found: {txt_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33829260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining overlap between chunks:\n",
      "\n",
      "\n",
      "Chunk 1 ending: \n",
      "   ...ontent: The actual text content\n",
      "   - metadata: Dictionary with additional information (source, page, date, etc.)\n",
      "   - id: Unique identifier (optional)\n",
      "\n",
      "Chunk 2 starting: \n",
      "   2. LCEL (LangChain Expression Language)\n",
      "   - Uses pipe operator | to chain components\n",
      "   - More readable than nested function calls\n",
      "   - Better error ...\n",
      "\n",
      "üí° Notice the overlap? This preserves context!\n"
     ]
    }
   ],
   "source": [
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "\n",
    "    splitter_with_overlap = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunks = splitter_with_overlap.split_documents(docs)\n",
    "    print(\"üîç Examining overlap between chunks:\\n\")\n",
    "\n",
    "    if len(chunks)>2:\n",
    "        chunk_1_end = chunks[0].page_content[-150:]\n",
    "        chunk_2_start = chunks[1].page_content[:150]\n",
    "\n",
    "        print(\"\\nChunk 1 ending: \")\n",
    "        print(f\"   ...{chunk_1_end}\")\n",
    "        print(\"\\nChunk 2 starting: \")\n",
    "        print(f\"   {chunk_2_start}...\")\n",
    "        print(\"\\nüí° Notice the overlap? This preserves context!\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3be18087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split code into 3 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "def calculate_total(items):\n",
      "    \"\"\"Calculate total price of items.\"\"\"\n",
      "    total = 0\n",
      "    for item in items:\n",
      "        total += item['price']\n",
      "    return total\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "def apply_discount(total, discount_percent):\n",
      "    \"\"\"Apply discount to total.\"\"\"\n",
      "    discount = total * (discount_percent / 100)\n",
      "    return total - discount\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "class ShoppingCart:\n",
      "    def __init__(self):\n",
      "        self.items = []\n",
      "\n",
      "    def add_item(self, item):\n",
      "        self.items.append(item)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "python_code = '''\n",
    "def calculate_total(items):\n",
    "    \"\"\"Calculate total price of items.\"\"\"\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total += item['price']\n",
    "    return total\n",
    "\n",
    "def apply_discount(total, discount_percent):\n",
    "    \"\"\"Apply discount to total.\"\"\"\n",
    "    discount = total * (discount_percent / 100)\n",
    "    return total - discount\n",
    "\n",
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "'''\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "code_chunk = python_splitter.split_text(python_code)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Split code into {len(code_chunk)} chunks:\\n\")\n",
    "for i, chunk in enumerate(code_chunk,1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7d0e668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First paragraph about machine learning.\\nIt has multiple sentences. This is important context.', 'Second paragraph about deep learning.\\nNeural networks are powerful. They learn from data.', 'Third paragraph about transformers.\\nAttention mechanisms are key. They revolutionized NLP.']\n",
      "Split into 3 chunks\n",
      "\n",
      "Chunk 1: \n",
      "First paragraph about machine learning.\n",
      "It has multiple sentences. This is important context.\n",
      "Chunk 2: \n",
      "Second paragraph about deep learning.\n",
      "Neural networks are powerful. They learn from data.\n",
      "Chunk 3: \n",
      "Third paragraph about transformers.\n",
      "Attention mechanisms are key. They revolutionized NLP.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Sample text with clear paragraph breaks\n",
    "sample_text = \"\"\"First paragraph about machine learning.\n",
    "It has multiple sentences. This is important context.\n",
    "\n",
    "Second paragraph about deep learning.\n",
    "Neural networks are powerful. They learn from data.\n",
    "\n",
    "Third paragraph about transformers.\n",
    "Attention mechanisms are key. They revolutionized NLP.\n",
    "\"\"\"\n",
    "\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "chunks = simple_splitter.split_text(sample_text)\n",
    "print(chunks)\n",
    "print(f\"Split into {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks,1):\n",
    "    print(f\"Chunk {i}: \\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658cfcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split HTML into 48 sections\n",
      "\n",
      "======================================================================\n",
      "Section 1\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content: Building Intelligent Applications with RAG...\n",
      "======================================================================\n",
      "Section 2\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content: | |  \n",
      "By Dr. Amanda Foster  \n",
      "January 15, 2025  \n",
      "12 min read...\n",
      "======================================================================\n",
      "Section 3\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Introduction'}\n",
      "Content: Introduction...\n",
      "======================================================================\n",
      "Section 4\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Introduction'}\n",
      "Content: In the rapidly evolving landscape of artificial intelligence, Retrieval-Augmented Generation (RAG) has emerged as a game-changing approach for building intelligent applications. Unlike traditional cha...\n",
      "======================================================================\n",
      "Section 5\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'What is Retrieval-Augmented Generation?'}\n",
      "Content: What is Retrieval-Augmented Generation?...\n",
      "======================================================================\n",
      "Section 6\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'What is Retrieval-Augmented Generation?'}\n",
      "Content: RAG is an architectural pattern that enhances language model outputs by incorporating relevant information retrieved from external knowledge sources. The process typically involves three key steps:  \n",
      "...\n",
      "======================================================================\n",
      "Section 7\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework'}\n",
      "Content: The LangChain Framework...\n",
      "======================================================================\n",
      "Section 8\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework'}\n",
      "Content: LangChain is an open-source framework that simplifies the development of LLM-powered applications. It provides a comprehensive suite of tools and abstractions for building RAG systems, including:...\n",
      "======================================================================\n",
      "Section 9\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Document Loaders'}\n",
      "Content: Document Loaders...\n",
      "======================================================================\n",
      "Section 10\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Document Loaders'}\n",
      "Content: LangChain supports loading documents from various sources: PDFs, web pages, databases, APIs, and more. Each loader handles format-specific parsing and converts content into a standardized Document obj...\n",
      "======================================================================\n",
      "Section 11\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Text Splitters'}\n",
      "Content: Text Splitters...\n",
      "======================================================================\n",
      "Section 12\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Text Splitters'}\n",
      "Content: Large documents need to be split into smaller chunks that fit within the context window of language models. LangChain offers sophisticated text splitters that respect semantic boundaries, such as the ...\n",
      "======================================================================\n",
      "Section 13\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Embeddings and Vector Stores'}\n",
      "Content: Embeddings and Vector Stores...\n",
      "======================================================================\n",
      "Section 14\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Embeddings and Vector Stores'}\n",
      "Content: To enable semantic search, text chunks are converted into vector embeddings using models like OpenAI's text-embedding-3 or open-source alternatives. These embeddings are stored in vector databases (FA...\n",
      "======================================================================\n",
      "Section 15\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Chains and LCEL'}\n",
      "Content: Chains and LCEL...\n",
      "======================================================================\n",
      "Section 16\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'The LangChain Framework', 'Subsection': 'Chains and LCEL'}\n",
      "Content: LangChain 1.0 introduced LCEL (LangChain Expression Language), a declarative way to compose components using a pipe operator. This makes it easy to build complex workflows while maintaining readabilit...\n",
      "======================================================================\n",
      "Section 17\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System'}\n",
      "Content: Building Your First RAG System...\n",
      "======================================================================\n",
      "Section 18\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System'}\n",
      "Content: Let's walk through the process of building a basic RAG system:...\n",
      "======================================================================\n",
      "Section 19\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 1: Load and Process Documents'}\n",
      "Content: Step 1: Load and Process Documents...\n",
      "======================================================================\n",
      "Section 20\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 1: Load and Process Documents'}\n",
      "Content: Start by loading your knowledge base documents and splitting them into manageable chunks. A common configuration is 1000-character chunks with 200-character overlap to maintain context across boundari...\n",
      "======================================================================\n",
      "Section 21\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 2: Create Embeddings'}\n",
      "Content: Step 2: Create Embeddings...\n",
      "======================================================================\n",
      "Section 22\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 2: Create Embeddings'}\n",
      "Content: Convert your text chunks into vector embeddings using an embedding model. Consider factors like cost, quality, and whether you need local/private processing when choosing a model....\n",
      "======================================================================\n",
      "Section 23\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 3: Set Up Vector Store'}\n",
      "Content: Step 3: Set Up Vector Store...\n",
      "======================================================================\n",
      "Section 24\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 3: Set Up Vector Store'}\n",
      "Content: Store your embeddings in a vector database. For development and testing, FAISS or Chroma work well. For production systems with large datasets, consider managed solutions like Pinecone or self-hosted ...\n",
      "======================================================================\n",
      "Section 25\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 4: Build the Retrieval Chain'}\n",
      "Content: Step 4: Build the Retrieval Chain...\n",
      "======================================================================\n",
      "Section 26\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 4: Build the Retrieval Chain'}\n",
      "Content: Create a chain that retrieves relevant documents for a query and passes them to a language model along with the query. Use LCEL to compose this pipeline elegantly....\n",
      "======================================================================\n",
      "Section 27\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 5: Test and Iterate'}\n",
      "Content: Step 5: Test and Iterate...\n",
      "======================================================================\n",
      "Section 28\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Building Your First RAG System', 'Subsection': 'Step 5: Test and Iterate'}\n",
      "Content: Evaluate your RAG system with diverse queries. Pay attention to retrieval quality (are the right documents being retrieved?) and generation quality (are the answers accurate and helpful?)....\n",
      "======================================================================\n",
      "Section 29\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations'}\n",
      "Content: Best Practices and Production Considerations...\n",
      "======================================================================\n",
      "Section 30\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Chunk Size Optimization'}\n",
      "Content: Chunk Size Optimization...\n",
      "======================================================================\n",
      "Section 31\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Chunk Size Optimization'}\n",
      "Content: There's no one-size-fits-all chunk size. Experiment with different sizes based on your content type. Technical documentation often works better with smaller chunks (500-800 characters), while narrativ...\n",
      "======================================================================\n",
      "Section 32\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Metadata Filtering'}\n",
      "Content: Metadata Filtering...\n",
      "======================================================================\n",
      "Section 33\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Metadata Filtering'}\n",
      "Content: Enrich your documents with metadata (source, date, category, author) and use it to filter search results. This can significantly improve retrieval precision for structured knowledge bases....\n",
      "======================================================================\n",
      "Section 34\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Hybrid Search'}\n",
      "Content: Hybrid Search...\n",
      "======================================================================\n",
      "Section 35\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Hybrid Search'}\n",
      "Content: Combine semantic (vector) search with keyword (BM25) search for more robust retrieval. Semantic search excels at understanding intent, while keyword search is better at exact matches and specific term...\n",
      "======================================================================\n",
      "Section 36\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Cost Management'}\n",
      "Content: Cost Management...\n",
      "======================================================================\n",
      "Section 37\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Cost Management'}\n",
      "Content: Monitor your embedding and LLM API costs. Consider caching frequently asked questions, using smaller/cheaper models where appropriate, and implementing rate limiting. Calculate costs before deploying ...\n",
      "======================================================================\n",
      "Section 38\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Evaluation Framework'}\n",
      "Content: Evaluation Framework...\n",
      "======================================================================\n",
      "Section 39\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Best Practices and Production Considerations', 'Subsection': 'Evaluation Framework'}\n",
      "Content: Build a test set of question-answer pairs and measure metrics like answer relevance, faithfulness (does the answer align with retrieved context?), and context precision (are retrieved documents releva...\n",
      "======================================================================\n",
      "Section 40\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques'}\n",
      "Content: Advanced RAG Techniques...\n",
      "======================================================================\n",
      "Section 41\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Query Transformation'}\n",
      "Content: Query Transformation...\n",
      "======================================================================\n",
      "Section 42\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Query Transformation'}\n",
      "Content: Transform user queries before retrieval to improve results. Techniques include query expansion, query decomposition (breaking complex queries into simpler sub-queries), and hypothetical document embed...\n",
      "======================================================================\n",
      "Section 43\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Re-ranking'}\n",
      "Content: Re-ranking...\n",
      "======================================================================\n",
      "Section 44\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Re-ranking'}\n",
      "Content: After initial retrieval, use a re-ranking model to reorder results based on relevance to the specific query. This two-stage approach (fast retrieval + quality ranking) offers better performance than s...\n",
      "======================================================================\n",
      "Section 45\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Adaptive Retrieval'}\n",
      "Content: Adaptive Retrieval...\n",
      "======================================================================\n",
      "Section 46\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Advanced RAG Techniques', 'Subsection': 'Adaptive Retrieval'}\n",
      "Content: Not all queries need retrieval. Implement logic to determine when to use RAG versus when the language model can answer from its training data alone. This reduces latency and costs for simple queries....\n",
      "======================================================================\n",
      "Section 47\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Conclusion'}\n",
      "Content: Conclusion...\n",
      "======================================================================\n",
      "Section 48\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Conclusion'}\n",
      "Content: RAG represents a significant advancement in building intelligent, reliable AI applications. By combining the strengths of retrieval systems and language models, we can create applications that are bot...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "html_path = \"sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    headers_to_split = [\n",
    "        (\"h1\",\"Title\"),\n",
    "        (\"h2\", \"Section\"),\n",
    "        (\"h3\", \"Subsection\"),\n",
    "    ]\n",
    "\n",
    "    html_splitter =HTMLHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split\n",
    "    )\n",
    "\n",
    "    html_chunks = html_splitter.split_text(html_content)\n",
    "    print(f\"‚úÇÔ∏è Split HTML into {len(html_chunks)} sections\\n\")\n",
    "\n",
    "    for i, chunk in enumerate(html_chunks,1):\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Section {i}\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå HTML file not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d78c9687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split JSON into 7 chunks\n",
      "\n",
      "First chunk:\n",
      "\"{\\\"api_version\\\": \\\"v2.0\\\", \\\"timestamp\\\": \\\"2025-01-15T10:30:00Z\\\", \\\"total_results\\\": 5, \\\"articles\\\": {\\\"0\\\": {\\\"id\\\": \\\"article_001\\\", \\\"title\\\": \\\"Introduction to Retrieval-Augmented Generation (RAG)\\\", \\\"author\\\": \\\"Dr. Sarah Chen\\\", \\\"published_date\\\": \\\"2025-01-10\\\", \\\"category\\\": \\\"Machine Learning\\\", \\\"tags\\\": {\\\"0\\\": \\\"RAG\\\", \\\"1\\\": \\\"LLM\\\", \\\"2\\\": \\\"NLP\\\", \\\"3\\\": \\\"AI\\\"}, \\\"summary\\\": \\\"Retrieval-Augmented Generation (RAG) is a powerful technique that combines information retrieval ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "json_path = \"sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Create splitter\n",
    "    json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "        min_chunk_size=100\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    json_chunks = json_splitter.split_text(\n",
    "        json_data=json_data,\n",
    "        convert_lists=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split JSON into {len(json_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show first chunk\n",
    "    print(\"First chunk:\")\n",
    "    print(json.dumps(json_chunks[0], indent=2)[:500] + \"...\")\n",
    "else:\n",
    "    print(f\"‚ùå JSON file not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37b2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 6 token-based chunks:\n",
      "\n",
      "Chunk 1: The transformer architecture, introduced in the paper 'Attention\n",
      "\n",
      "Chunk 2:  'Attention Is All You Need', \n",
      "revolutionized\n",
      "\n",
      "Chunk 3: revolutionized natural language processing. It uses self-\n",
      "\n",
      "Chunk 4:  self-attention mechanisms to process \n",
      "sequences in parallel\n",
      "\n",
      "Chunk 5:  in parallel, making it much faster than recurrent neural\n",
      "\n",
      "Chunk 6:  recurrent neural networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
    "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
    "sequences in parallel, making it much faster than recurrent neural networks.\"\"\"\n",
    "\n",
    "# Token-based splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=10,  # 50 tokens (not characters!)\n",
    "    chunk_overlap=2,\n",
    "    encoding_name=\"cl100k_base\"  # GPT-3.5/GPT-4 tokenizer\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "\n",
    "print(f\"Split into {len(token_chunks)} token-based chunks:\\n\")\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1584246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
